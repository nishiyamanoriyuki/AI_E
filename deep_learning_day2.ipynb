{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.3"},"colab":{"name":"deep_learning_day2.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true}},"cells":[{"cell_type":"markdown","metadata":{"id":"8cNl2QA_Rnv5"},"source":["# 要点とまとめ"]},{"cell_type":"markdown","source":["section1:勾配消失問題\n","要点まとめ\n","勾配消失問題とは、下位層がパラメータはほとんど変わらず、最適値収束しなくなることである。\n","これは誤差逆伝播法が下位層に進んでいくに連れて、勾配がどんどん緩やかになっていくためである。\n","対策としては、活性化関数の選択、重みの初期値設定、バッチ正規化がある。\n","具体的にはrelu関数を使用する、xavierやhe(重みの要素を、前の層のノード数の平方根で除算等)の初期化法、バッチ正規化を行う。\n","\n","\n","section２:学習率最適化手法\n","要点まとめ\n","学習率最適化手法としてモメンタム•AdaGrad•RMSProp•Adamがある。\n","モーメンタムは局所的最適解にはならず、大域的最適解となる。・谷間についてから最も低い位置(最適値)にいくまでの時間が早い。\n","AdaGradは誤差を学習率が徐々に小さくなるので、鞍点問題を引き起こす。\n","RMSPropはハイパーパラメータの調整が必要な場合が少ない。\n","AdamはモメンタムおよびRMSPropのメリットをもっている。\n","\n","section３:過学習\n","要点まとめ\n","過学習は重みが大きい値をとることで、過学習が発生することがある。\n","過学習抑制手法として、L1正則化、L2正則化、ドロップアウトがある。\n","L1正則化（ラッソ）は、ベクトル成分の絶対値の和(マンハッタン距離と呼ばれる)を使用し、L2正則化（リッジ）はユーグリッド距離を使用する。どちらも誤差関数にノルムを加える。\n","ドロップアウトはノードの数が多いことで起こる過学習を抑制する。\n","\n","\n","\n","section４:畳み込みニューラルネットワークの概念\n","要点まとめ\n","CNNは畳み込み層とプーリング層によって構成される。\n","畳み込み層では、画像の場合、縦、横、チャンネルの3次元のデータをそのまま学習し、次に伝えることができる。\n","フィルタをいくつずつ動かすかいう意味のストライド数や、端側のどのように埋めるかというパディング等を設定する。\n","プーリング層では対象領域の最大値や平均値を取得する。\n","\n","section５:最新のCNN\n","要点まとめ\n","CNNの代表レイヤーとしてAlexNetが存在する。\n","AlexNetが2012年の画像分類チャレンジコンテスト（ILSVRC）において飛躍的な成績をあげ、ディープラーニングが注目される要因の一つとなった。\n","3 つの畳み込み層（convolutional layer）、2 つのプーリング層（pooling layer）および 3 つの全結合層（fully connected layer）からなり、最終層にドロップアウトを採用している。\n"],"metadata":{"id":"rUDw6usChccp"}},{"cell_type":"markdown","metadata":{"id":"3Ub7RYdeY6pK"},"source":["## 実装演習"]},{"cell_type":"code","metadata":{"id":"7Ic2JzkvFX59","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639949238041,"user_tz":-540,"elapsed":18844,"user":{"displayName":"西山憲之","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02386897622438755758"}},"outputId":"3708638d-16fe-4975-81dd-91b5822d83bf"},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","import sys\n","sys.path.append('/content/drive/My Drive/DNN_code')\n","sys.path.append('/content/drive/My Drive/DNN_code/lesson_2')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"id":"N0Ys-3jbfBdJ"},"source":["import numpy as np\n","from common import layers\n","from collections import OrderedDict\n","from common import functions\n","from data.mnist import load_mnist\n","import matplotlib.pyplot as plt\n","\n","\n","# ReLU layer\n","class Relu:\n","    def __init__(self):\n","        self.mask = None\n","\n","    def forward(self, x):\n","        # mask.shape = x.shape\n","        # True or Falseを要素として持つ\n","        self.mask = (x <= 0)\n","        out = x.copy()\n","        # Trueの箇所を0にする\n","        out[self.mask] = 0\n","\n","        return out\n","\n","    def backward(self, dout):\n","        # Trueの箇所を0にする\n","        dout[self.mask] = 0\n","        dx = dout\n","\n","        return dx\n","    \n","# Affine layer(全結合 layer)\n","class Affine:\n","    \n","    def __init__(self, W, b):\n","        self.W =W\n","        self.b = b\n","        \n","        self.x = None\n","        self.original_x_shape = None\n","        # 重み・バイアスパラメータの微分\n","        self.dW = None\n","        self.db = None\n","\n","    def forward(self, x):\n","        out = np.dot(self.x, self.W) + self.b\n","        \n","        return out\n","\n","    def backward(self, dout):\n","        dx = np.dot(dout, self.W.T)\n","        self.dW = np.dot(self.x.T, dout)\n","        self.db = np.sum(dout, axis=0)\n","\n","        return dx\n","\n","class TwoLayerNet:\n","    '''\n","    input_size: 入力層のノード数\n","    hidden_size: 隠れ層のノード数\n","    output_size: 出力層のノード数\n","    weight_init_std: 重みの初期化方法\n","    '''\n","    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\n","        # 重みの初期化\n","        self.params = {}\n","        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n","        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n","        self.params['b1'] = np.zeros(hidden_size)\n","        self.params['b2'] = np.zeros(output_size)\n","\n","        # レイヤの生成\n","        self.layers = OrderedDict()\n","        self.layers['Affine1'] = layers.Affine(self.params['W1'], self.params['b1'])\n","        self.layers['Relu1'] = layers.Relu()\n","        self.layers['Affine2'] = layers.Affine(self.params['W2'], self.params['b2'])\n","        \n","        self.lastLayer = layers.SoftmaxWithLoss()\n","        \n","    # 順伝播\n","    def predict(self, x):\n","        for layer in self.layers.values():\n","            x = layer.forward(x)\n","        \n","        return x\n","        \n","    # 誤差\n","    def loss(self, x, d):\n","        y = self.predict(x)\n","        return self.lastLayer.forward(y, d)\n","    \n","    # 精度\n","    def accuracy(self, x, d):\n","        y = self.predict(x)\n","        y = np.argmax(y, axis=1)\n","        if d.ndim != 1 : d = np.argmax(d, axis=1)\n","        \n","        accuracy = np.sum(y == d) / float(x.shape[0])\n","        return accuracy\n","             \n","    # 勾配\n","    def gradient(self, x, d):\n","        # forward\n","\n","        self.loss(x, d)\n","\n","        # backward\n","        dout = 1\n","        dout = self.lastLayer.backward(dout)\n","\n","        layers = list(self.layers.values())\n","        layers.reverse()\n","        for layer in layers:\n","            dout = layer.backward(dout)\n","\n","        # 設定\n","        grad = {}\n","        grad['W1'], grad['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n","        grad['W2'], grad['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n","\n","        return grad\n","\n","# データの読み込み\n","(x_train, d_train), (x_test, d_test) = load_mnist(normalize=True, one_hot_label=True)\n","\n","print(\"データ読み込み完了\")\n","\n","network = TwoLayerNet(input_size=784, hidden_size=40, output_size=10)\n","\n","iters_num = 1000\n","train_size = x_train.shape[0]\n","batch_size = 100\n","learning_rate = 0.1\n","\n","train_loss_list = []\n","accuracies_train = []\n","accuracies_test = []\n","\n","plot_interval=10\n","\n","for i in range(iters_num):\n","    batch_mask = np.random.choice(train_size, batch_size)\n","    x_batch = x_train[batch_mask]\n","    d_batch = d_train[batch_mask]\n","\n","    # 勾配\n","    grad = network.gradient(x_batch, d_batch)\n","    \n","    for key in ('W1', 'W2', 'b1', 'b2'):\n","        network.params[key] -= learning_rate * grad[key]\n","    \n","    loss = network.loss(x_batch, d_batch)\n","    train_loss_list.append(loss)\n","    \n","    if (i + 1) % plot_interval == 0:\n","        accr_test = network.accuracy(x_test, d_test)\n","        accuracies_test.append(accr_test)        \n","        accr_train = network.accuracy(x_batch, d_batch)\n","        accuracies_train.append(accr_train)\n","\n","        print('Generation: ' + str(i+1) + '. 正答率(トレーニング) = ' + str(accr_train))\n","        print('                : ' + str(i+1) + '. 正答率(テスト) = ' + str(accr_test))\n","        \n","\n","lists = range(0, iters_num, plot_interval)\n","plt.plot(lists, accuracies_train, label=\"training set\")\n","plt.plot(lists, accuracies_test,  label=\"test set\")\n","plt.legend(loc=\"lower right\")\n","plt.title(\"accuracy\")\n","plt.xlabel(\"count\")\n","plt.ylabel(\"accuracy\")\n","plt.ylim(0, 1.0)\n","# グラフの表示\n","plt.show()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","from common import layers\n","from collections import OrderedDict\n","from common import functions\n","from data.mnist import load_mnist\n","import matplotlib.pyplot as plt\n","\n","\n","class MultiLayerNet:\n","    '''\n","    input_size: 入力層のノード数\n","    hidden_size_list: 隠れ層のノード数のリスト\n","    output_size: 出力層のノード数\n","    activation: 活性化関数\n","    weight_init_std: 重みの初期化方法\n","    '''\n","    def __init__(self, input_size, hidden_size_list, output_size, activation='relu', weight_init_std='relu'):\n","        self.input_size = input_size\n","        self.output_size = output_size\n","        self.hidden_size_list = hidden_size_list\n","        self.hidden_layer_num = len(hidden_size_list)\n","        self.params = {}\n","\n","        # 重みの初期化\n","        self.__init_weight(weight_init_std)\n","\n","        # レイヤの生成, sigmoidとreluのみ扱う\n","        activation_layer = {'sigmoid': layers.Sigmoid, 'relu': layers.Relu}\n","        self.layers = OrderedDict() # 追加した順番に格納\n","        for idx in range(1, self.hidden_layer_num+1):\n","            self.layers['Affine' + str(idx)] = layers.Affine(self.params['W' + str(idx)], self.params['b' + str(idx)])\n","            self.layers['Activation_function' + str(idx)] = activation_layer[activation]()\n","\n","        idx = self.hidden_layer_num + 1\n","        self.layers['Affine' + str(idx)] = layers.Affine(self.params['W' + str(idx)], self.params['b' + str(idx)])\n","\n","        self.last_layer = layers.SoftmaxWithLoss()\n","\n","    def __init_weight(self, weight_init_std):\n","        all_size_list = [self.input_size] + self.hidden_size_list + [self.output_size]\n","        for idx in range(1, len(all_size_list)):\n","            scale = weight_init_std\n","            if str(weight_init_std).lower() in ('relu', 'he'):\n","                scale = np.sqrt(2.0 / all_size_list[idx - 1])\n","            elif str(weight_init_std).lower() in ('sigmoid', 'xavier'):\n","                scale = np.sqrt(1.0 / all_size_list[idx - 1])\n","\n","            self.params['W' + str(idx)] = scale * np.random.randn(all_size_list[idx-1], all_size_list[idx])\n","            self.params['b' + str(idx)] = np.zeros(all_size_list[idx])\n","\n","    def predict(self, x):\n","        for layer in self.layers.values():\n","            x = layer.forward(x)\n","\n","        return x\n","\n","    def loss(self, x, d):\n","        y = self.predict(x)\n","\n","        weight_decay = 0\n","        for idx in range(1, self.hidden_layer_num + 2):\n","            W = self.params['W' + str(idx)]\n","\n","        return self.last_layer.forward(y, d) + weight_decay\n","\n","    def accuracy(self, x, d):\n","        y = self.predict(x)\n","        y = np.argmax(y, axis=1)\n","        if d.ndim != 1 : d = np.argmax(d, axis=1)\n","\n","        accuracy = np.sum(y == d) / float(x.shape[0])\n","        return accuracy\n","\n","    def gradient(self, x, d):\n","        # forward\n","        self.loss(x, d)\n","\n","        # backward\n","        dout = 1\n","        dout = self.last_layer.backward(dout)\n","\n","        layers = list(self.layers.values())\n","        layers.reverse()\n","        for layer in layers:\n","            dout = layer.backward(dout)\n","\n","        # 設定\n","        grad = {}\n","        for idx in range(1, self.hidden_layer_num+2):\n","            grad['W' + str(idx)] = self.layers['Affine' + str(idx)].dW\n","            grad['b' + str(idx)] = self.layers['Affine' + str(idx)].db\n","\n","        return grad\n","\n","# データの読み込み\n","(x_train, d_train), (x_test, d_test) = load_mnist(normalize=True, one_hot_label=True)\n","\n","print(\"データ読み込み完了\")\n","\n","#勾配消失sample\n","network = MultiLayerNet(input_size=784, hidden_size_list=[40, 20], output_size=10, activation='sigmoid', weight_init_std=0.01)\n","#relu_gauss\n","#network = MultiLayerNet(input_size=784, hidden_size_list=[40, 20], output_size=10, activation='relu', weight_init_std=0.01)\n","#シグモイド_Xaivier\n","#network = MultiLayerNet(input_size=784, hidden_size_list=[40, 20], output_size=10, activation='sigmoid', weight_init_std='Xavier')\n","#relu_he\n","#network = MultiLayerNet(input_size=784, hidden_size_list=[40, 20], output_size=10, activation='relu', weight_init_std='He')\n","\n","iters_num = 2000\n","train_size = x_train.shape[0]\n","batch_size = 100\n","learning_rate = 0.1\n","\n","train_loss_list = []\n","accuracies_train = []\n","accuracies_test = []\n","\n","plot_interval=10\n","\n","for i in range(iters_num):\n","    batch_mask = np.random.choice(train_size, batch_size)\n","    x_batch = x_train[batch_mask]\n","    d_batch = d_train[batch_mask]\n","\n","    # 勾配\n","    grad = network.gradient(x_batch, d_batch)\n","    \n","    for key in ('W1', 'W2', 'W3', 'b1', 'b2', 'b3'):\n","        network.params[key] -= learning_rate * grad[key]\n","    \n","    loss = network.loss(x_batch, d_batch)\n","    train_loss_list.append(loss)\n","    \n","    if (i + 1) % plot_interval == 0:\n","        accr_test = network.accuracy(x_test, d_test)\n","        accuracies_test.append(accr_test)        \n","        accr_train = network.accuracy(x_batch, d_batch)\n","        accuracies_train.append(accr_train)\n","\n","        print('Generation: ' + str(i+1) + '. 正答率(トレーニング) = ' + str(accr_train))\n","        print('                : ' + str(i+1) + '. 正答率(テスト) = ' + str(accr_test))\n","        \n","\n","lists = range(0, iters_num, plot_interval)\n","plt.plot(lists, accuracies_train, label=\"training set\")\n","plt.plot(lists, accuracies_test,  label=\"test set\")\n","plt.legend(loc=\"lower right\")\n","plt.title(\"accuracy\")\n","plt.xlabel(\"count\")\n","plt.ylabel(\"accuracy\")\n","plt.ylim(0, 1.0)\n","# グラフの表示\n","plt.show()"],"metadata":{"id":"YlHynkrO1_jl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","from collections import OrderedDict\n","from common import layers\n","from data.mnist import load_mnist\n","import matplotlib.pyplot as plt\n","from multi_layer_net import MultiLayerNet\n","from common import optimizer\n","\n","# バッチ正則化 layer\n","class BatchNormalization:\n","    '''\n","    gamma: スケール係数\n","    beta: オフセット\n","    momentum: 慣性\n","    running_mean: テスト時に使用する平均\n","    running_var: テスト時に使用する分散\n","    '''\n","    def __init__(self, gamma, beta, momentum=0.9, running_mean=None, running_var=None):\n","        self.gamma = gamma\n","        self.beta = beta\n","        self.momentum = momentum\n","        self.input_shape = None\n","\n","        self.running_mean = running_mean\n","        self.running_var = running_var  \n","        \n","        # backward時に使用する中間データ\n","        self.batch_size = None\n","        self.xc = None\n","        self.std = None\n","        self.dgamma = None\n","        self.dbeta = None\n","\n","    def forward(self, x, train_flg=True):\n","        if self.running_mean is None:\n","            N, D = x.shape\n","            self.running_mean = np.zeros(D)\n","            self.running_var = np.zeros(D)\n","                        \n","        if train_flg:\n","            mu = x.mean(axis=0) # 平均\n","            xc = x - mu # xをセンタリング\n","            var = np.mean(xc**2, axis=0) # 分散\n","            std = np.sqrt(var + 10e-7) # スケーリング\n","            xn = xc / std\n","            \n","            self.batch_size = x.shape[0]\n","            self.xc = xc\n","            self.xn = xn\n","            self.std = std\n","            self.running_mean = self.momentum * self.running_mean + (1-self.momentum) * mu # 平均値の加重平均\n","            self.running_var = self.momentum * self.running_var + (1-self.momentum) * var #分散値の加重平均\n","        else:\n","            xc = x - self.running_mean\n","            xn = xc / ((np.sqrt(self.running_var + 10e-7)))\n","            \n","        out = self.gamma * xn + self.beta \n","        \n","        return out\n","\n","    def backward(self, dout):\n","        dbeta = dout.sum(axis=0)\n","        dgamma = np.sum(self.xn * dout, axis=0)\n","        dxn = self.gamma * dout\n","        dxc = dxn / self.std\n","        dstd = -np.sum((dxn * self.xc) / (self.std * self.std), axis=0)\n","        dvar = 0.5 * dstd / self.std\n","        dxc += (2.0 / self.batch_size) * self.xc * dvar\n","        dmu = np.sum(dxc, axis=0)\n","        dx = dxc - dmu / self.batch_size\n","        \n","        self.dgamma = dgamma\n","        self.dbeta = dbeta\n","\n","        return dx    \n","\n","(x_train, d_train), (x_test, d_test) = load_mnist(normalize=True)\n","\n","print(\"データ読み込み完了\")\n","\n","\n","# batch_normalizationの設定 =======================\n","use_batchnorm = True\n","# use_batchnorm = False\n","# ====================================================\n","\n","network = MultiLayerNet(input_size=784, hidden_size_list=[40, 20], output_size=10,\n","                        activation='sigmoid', weight_init_std='Xavier', use_batchnorm=use_batchnorm)\n","\n","iters_num = 1000\n","train_size = x_train.shape[0]\n","batch_size = 100\n","learning_rate=0.01\n","\n","train_loss_list = []\n","accuracies_train = []\n","accuracies_test = []\n","\n","plot_interval=10\n","\n","\n","for i in range(iters_num):\n","    batch_mask = np.random.choice(train_size, batch_size)\n","    x_batch = x_train[batch_mask]\n","    d_batch = d_train[batch_mask]\n","\n","    grad = network.gradient(x_batch, d_batch)\n","    for key in ('W1', 'W2', 'W3', 'b1', 'b2', 'b3'):\n","        network.params[key] -= learning_rate * grad[key]\n","\n","        loss = network.loss(x_batch, d_batch)\n","        train_loss_list.append(loss)        \n","        \n","    if (i + 1) % plot_interval == 0:\n","        accr_test = network.accuracy(x_test, d_test)\n","        accuracies_test.append(accr_test)        \n","        accr_train = network.accuracy(x_batch, d_batch)\n","        accuracies_train.append(accr_train)\n","        \n","        print('Generation: ' + str(i+1) + '. 正答率(トレーニング) = ' + str(accr_train))\n","        print('                : ' + str(i+1) + '. 正答率(テスト) = ' + str(accr_test))\n","                \n","\n","lists = range(0, iters_num, plot_interval)\n","plt.plot(lists, accuracies_train, label=\"training set\")\n","plt.plot(lists, accuracies_test,  label=\"test set\")\n","plt.legend(loc=\"lower right\")\n","plt.title(\"accuracy\")\n","plt.xlabel(\"count\")\n","plt.ylabel(\"accuracy\")\n","plt.ylim(0, 1.0)\n","# グラフの表示\n","plt.show()"],"metadata":{"id":"IxV4smZUAzvL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","from collections import OrderedDict\n","from common import layers\n","from data.mnist import load_mnist\n","import matplotlib.pyplot as plt\n","from multi_layer_net import MultiLayerNet\n","\n","\n","# データの読み込み\n","(x_train, d_train), (x_test, d_test) = load_mnist(normalize=True, one_hot_label=True)\n","\n","print(\"データ読み込み完了\")\n","\n","# batch_normalizationの設定 =======================\n","# use_batchnorm = True\n","use_batchnorm = False\n","# ====================================================\n","\n","\n","network = MultiLayerNet(input_size=784, hidden_size_list=[40, 20], output_size=10, activation='sigmoid', weight_init_std=0.01,\n","                       use_batchnorm=use_batchnorm)\n","\n","iters_num = 1000\n","train_size = x_train.shape[0]\n","batch_size = 100\n","learning_rate = 0.01\n","\n","train_loss_list = []\n","accuracies_train = []\n","accuracies_test = []\n","\n","plot_interval=10\n","\n","for i in range(iters_num):\n","    batch_mask = np.random.choice(train_size, batch_size)\n","    x_batch = x_train[batch_mask]\n","    d_batch = d_train[batch_mask]\n","\n","    # 勾配\n","    \n","    grad = network.gradient(x_batch, d_batch)\n","    #sgd\n","    \n","    for key in ('W1', 'W2', 'W3', 'b1', 'b2', 'b3'):\n","        network.params[key] -= learning_rate * grad[key]\n","        \n","    # Momentum\n","    #learning_rate = 0.3\n","    #momentum = 0.9\n","    #for key in ('W1', 'W2', 'W3', 'b1', 'b2', 'b3'):\n","    #    if i == 0:\n","    #        v[key] = np.zeros_like(network.params[key])\n","    #    v[key] = momentum * v[key] - learning_rate * grad[key]\n","    #    network.params[key] += v[key]\n","    \n","    #Adagrad\n","    #learning_rate = 0.1\n","    #if i == 0:\n","    #    h = {}\n","    #for key in ('W1', 'W2', 'W3', 'b1', 'b2', 'b3'):\n","    #    if i == 0:\n","    #        h[key] = np.full_like(network.params[key], 1e-4)\n","    #    else:\n","    #        h[key] += np.square(grad[key])\n","    #    network.params[key] -= learning_rate * grad[key] / (np.sqrt(h[key]))\n","\n","    #RMSProp\n","    #learning_rate = 0.01\n","    #decay_rate = 0.99\n","    #if i == 0:\n","    #    h = {}\n","    #for key in ('W1', 'W2', 'W3', 'b1', 'b2', 'b3'):\n","    #    if i == 0:\n","    #        h[key] = np.zeros_like(network.params[key])\n","    #    h[key] *= decay_rate\n","    #    h[key] += (1 - decay_rate) * np.square(grad[key])\n","    #    network.params[key] -= learning_rate * grad[key] / (np.sqrt(h[key]) + 1e-7)\n","\n","    #Adam\n","    #learning_rate = 0.01\n","    #beta1 = 0.9\n","    #beta2 = 0.999\n","    #if i == 0:\n","    #    m = {}\n","    #    v = {}\n","    #learning_rate_t  = learning_rate * np.sqrt(1.0 - beta2 ** (i + 1)) / (1.0 - beta1 ** (i + 1))    \n","    #for key in ('W1', 'W2', 'W3', 'b1', 'b2', 'b3'):\n","    #    if i == 0:\n","    #        m[key] = np.zeros_like(network.params[key])\n","    #        v[key] = np.zeros_like(network.params[key])\n","            \n","    #    m[key] += (1 - beta1) * (grad[key] - m[key])\n","    #    v[key] += (1 - beta2) * (grad[key] ** 2 - v[key])            \n","    #    network.params[key] -= learning_rate_t * m[key] / (np.sqrt(v[key]) + 1e-7)\n","\n","        loss = network.loss(x_batch, d_batch)\n","        train_loss_list.append(loss)\n","\n","    if (i + 1) % plot_interval == 0:\n","        accr_test = network.accuracy(x_test, d_test)\n","        accuracies_test.append(accr_test)        \n","        accr_train = network.accuracy(x_batch, d_batch)\n","        accuracies_train.append(accr_train)\n","        \n","        print('Generation: ' + str(i+1) + '. 正答率(トレーニング) = ' + str(accr_train))\n","        print('                : ' + str(i+1) + '. 正答率(テスト) = ' + str(accr_test))\n","\n","        \n","lists = range(0, iters_num, plot_interval)\n","plt.plot(lists, accuracies_train, label=\"training set\")\n","plt.plot(lists, accuracies_test,  label=\"test set\")\n","plt.legend(loc=\"lower right\")\n","plt.title(\"accuracy\")\n","plt.xlabel(\"count\")\n","plt.ylabel(\"accuracy\")\n","plt.ylim(0, 1.0)\n","# グラフの表示\n","plt.show()"],"metadata":{"id":"KrJo0AndHogR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","from collections import OrderedDict\n","from common import layers\n","from data.mnist import load_mnist\n","import matplotlib.pyplot as plt\n","from multi_layer_net import MultiLayerNet\n","from common import optimizer\n","\n","\n","(x_train, d_train), (x_test, d_test) = load_mnist(normalize=True)\n","\n","print(\"データ読み込み完了\")\n","\n","# 過学習を再現するために、学習データを削減\n","x_train = x_train[:300]\n","d_train = d_train[:300]\n","\n","network = MultiLayerNet(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100], output_size=10)\n","optimizer = optimizer.SGD(learning_rate=0.01)\n","\n","iters_num = 1000\n","train_size = x_train.shape[0]\n","batch_size = 100\n","\n","train_loss_list = []\n","accuracies_train = []\n","accuracies_test = []\n","\n","plot_interval=10\n","\n","\n","for i in range(iters_num):\n","    batch_mask = np.random.choice(train_size, batch_size)\n","    x_batch = x_train[batch_mask]\n","    d_batch = d_train[batch_mask]\n","\n","    grad = network.gradient(x_batch, d_batch)\n","    optimizer.update(network.params, grad)\n","\n","    loss = network.loss(x_batch, d_batch)\n","    train_loss_list.append(loss)\n","        \n","    if (i+1) % plot_interval == 0:\n","        accr_train = network.accuracy(x_train, d_train)\n","        accr_test = network.accuracy(x_test, d_test)\n","        accuracies_train.append(accr_train)\n","        accuracies_test.append(accr_test)\n","\n","        print('Generation: ' + str(i+1) + '. 正答率(トレーニング) = ' + str(accr_train))\n","        print('                : ' + str(i+1) + '. 正答率(テスト) = ' + str(accr_test))        \n","\n","lists = range(0, iters_num, plot_interval)\n","plt.plot(lists, accuracies_train, label=\"training set\")\n","plt.plot(lists, accuracies_test,  label=\"test set\")\n","plt.legend(loc=\"lower right\")\n","plt.title(\"accuracy\")\n","plt.xlabel(\"count\")\n","plt.ylabel(\"accuracy\")\n","plt.ylim(0, 1.0)\n","# グラフの表示\n","plt.show()"],"metadata":{"id":"6d1or5ekSsRT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from common import optimizer\n","\n","(x_train, d_train), (x_test, d_test) = load_mnist(normalize=True)\n","\n","print(\"データ読み込み完了\")\n","\n","# 過学習を再現するために、学習データを削減\n","x_train = x_train[:300]\n","d_train = d_train[:300]\n","\n","\n","network = MultiLayerNet(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100], output_size=10)\n","\n","\n","iters_num = 1000\n","train_size = x_train.shape[0]\n","batch_size = 100\n","learning_rate=0.01\n","\n","train_loss_list = []\n","accuracies_train = []\n","accuracies_test = []\n","\n","plot_interval=10\n","hidden_layer_num = network.hidden_layer_num\n","\n","# 正則化強度設定 ======================================\n","#l2\n","weight_decay_lambda = 0.1\n","#l1\n","#weight_decay_lambda = 0.005\n","# =================================================\n","\n","for i in range(iters_num):\n","    batch_mask = np.random.choice(train_size, batch_size)\n","    x_batch = x_train[batch_mask]\n","    d_batch = d_train[batch_mask]\n","\n","    grad = network.gradient(x_batch, d_batch)\n","    weight_decay = 0\n","    \n","    for idx in range(1, hidden_layer_num+1):\n","        grad['W' + str(idx)] = network.layers['Affine' + str(idx)].dW + weight_decay_lambda * network.params['W' + str(idx)]\n","        grad['b' + str(idx)] = network.layers['Affine' + str(idx)].db\n","        network.params['W' + str(idx)] -= learning_rate * grad['W' + str(idx)]\n","        network.params['b' + str(idx)] -= learning_rate * grad['b' + str(idx)]        \n","        #l2\n","        weight_decay += 0.5 * weight_decay_lambda * np.sqrt(np.sum(network.params['W' + str(idx)] ** 2))\n","        #l1\n","        #weight_decay += weight_decay_lambda * np.sum(np.abs(network.params['W' + str(idx)]))\n","\n","    loss = network.loss(x_batch, d_batch) + weight_decay\n","    train_loss_list.append(loss)        \n","        \n","    if (i+1) % plot_interval == 0:\n","        accr_train = network.accuracy(x_train, d_train)\n","        accr_test = network.accuracy(x_test, d_test)\n","        accuracies_train.append(accr_train)\n","        accuracies_test.append(accr_test)\n","        \n","        print('Generation: ' + str(i+1) + '. 正答率(トレーニング) = ' + str(accr_train))\n","        print('                : ' + str(i+1) + '. 正答率(テスト) = ' + str(accr_test))               \n","\n","\n","lists = range(0, iters_num, plot_interval)\n","plt.plot(lists, accuracies_train, label=\"training set\")\n","plt.plot(lists, accuracies_test,  label=\"test set\")\n","plt.legend(loc=\"lower right\")\n","plt.title(\"accuracy\")\n","plt.xlabel(\"count\")\n","plt.ylabel(\"accuracy\")\n","plt.ylim(0, 1.0)\n","# グラフの表示\n","plt.show()"],"metadata":{"id":"dvz76XdfUDq_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Dropout:\n","    def __init__(self, dropout_ratio=0.5):\n","        self.dropout_ratio = dropout_ratio\n","        self.mask = None\n","\n","    def forward(self, x, train_flg=True):\n","        if train_flg:\n","            self.mask = np.random.rand(*x.shape) > self.dropout_ratio\n","            return x * self.mask\n","        else:\n","            return x * (1.0 - self.dropout_ratio)\n","\n","    def backward(self, dout):\n","        return dout * self.mask\n","\n","from common import optimizer\n","(x_train, d_train), (x_test, d_test) = load_mnist(normalize=True)\n","\n","print(\"データ読み込み完了\")\n","\n","# 過学習を再現するために、学習データを削減\n","x_train = x_train[:300]\n","d_train = d_train[:300]\n","\n","# ドロップアウト設定 ======================================\n","use_dropout = True\n","dropout_ratio = 0.15\n","# ====================================================\n","\n","network = MultiLayerNet(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100], output_size=10,\n","                        weight_decay_lambda=weight_decay_lambda, use_dropout = use_dropout, dropout_ratio = dropout_ratio)\n","optimizer = optimizer.SGD(learning_rate=0.01)\n","# optimizer = optimizer.Momentum(learning_rate=0.01, momentum=0.9)\n","# optimizer = optimizer.AdaGrad(learning_rate=0.01)\n","# optimizer = optimizer.Adam()\n","\n","iters_num = 1000\n","train_size = x_train.shape[0]\n","batch_size = 100\n","\n","train_loss_list = []\n","accuracies_train = []\n","accuracies_test = []\n","\n","plot_interval=10\n","\n","\n","for i in range(iters_num):\n","    batch_mask = np.random.choice(train_size, batch_size)\n","    x_batch = x_train[batch_mask]\n","    d_batch = d_train[batch_mask]\n","\n","    grad = network.gradient(x_batch, d_batch)\n","    optimizer.update(network.params, grad)\n","\n","    loss = network.loss(x_batch, d_batch)\n","    train_loss_list.append(loss)    \n","    \n","    if (i+1) % plot_interval == 0:\n","        accr_train = network.accuracy(x_train, d_train)\n","        accr_test = network.accuracy(x_test, d_test)\n","        accuracies_train.append(accr_train)\n","        accuracies_test.append(accr_test)\n","\n","        print('Generation: ' + str(i+1) + '. 正答率(トレーニング) = ' + str(accr_train))\n","        print('                : ' + str(i+1) + '. 正答率(テスト) = ' + str(accr_test))        \n","        \n","lists = range(0, iters_num, plot_interval)\n","plt.plot(lists, accuracies_train, label=\"training set\")\n","plt.plot(lists, accuracies_test,  label=\"test set\")\n","plt.legend(loc=\"lower right\")\n","plt.title(\"accuracy\")\n","plt.xlabel(\"count\")\n","plt.ylabel(\"accuracy\")\n","plt.ylim(0, 1.0)\n","# グラフの表示\n","plt.show()"],"metadata":{"id":"dOgU6bLRUuO8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from common import optimizer\n","(x_train, d_train), (x_test, d_test) = load_mnist(normalize=True)\n","\n","print(\"データ読み込み完了\")\n","\n","# 過学習を再現するために、学習データを削減\n","x_train = x_train[:300]\n","d_train = d_train[:300]\n","\n","# ドロップアウト設定 ======================================\n","use_dropout = True\n","dropout_ratio = 0.08\n","# ====================================================\n","\n","network = MultiLayerNet(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100], output_size=10,\n","                        use_dropout = use_dropout, dropout_ratio = dropout_ratio)\n","\n","iters_num = 1000\n","train_size = x_train.shape[0]\n","batch_size = 100\n","learning_rate=0.01\n","\n","train_loss_list = []\n","accuracies_train = []\n","accuracies_test = []\n","hidden_layer_num = network.hidden_layer_num\n","\n","plot_interval=10\n","\n","# 正則化強度設定 ======================================\n","weight_decay_lambda=0.004\n","# =================================================\n","\n","for i in range(iters_num):\n","    batch_mask = np.random.choice(train_size, batch_size)\n","    x_batch = x_train[batch_mask]\n","    d_batch = d_train[batch_mask]\n","\n","    grad = network.gradient(x_batch, d_batch)\n","    weight_decay = 0\n","    \n","    for idx in range(1, hidden_layer_num+1):\n","        grad['W' + str(idx)] = network.layers['Affine' + str(idx)].dW + weight_decay_lambda * np.sign(network.params['W' + str(idx)])\n","        grad['b' + str(idx)] = network.layers['Affine' + str(idx)].db\n","        network.params['W' + str(idx)] -= learning_rate * grad['W' + str(idx)]\n","        network.params['b' + str(idx)] -= learning_rate * grad['b' + str(idx)]        \n","        weight_decay += weight_decay_lambda * np.sum(np.abs(network.params['W' + str(idx)]))\n","\n","    loss = network.loss(x_batch, d_batch) + weight_decay\n","    train_loss_list.append(loss)        \n","        \n","    if (i+1) % plot_interval == 0:\n","        accr_train = network.accuracy(x_train, d_train)\n","        accr_test = network.accuracy(x_test, d_test)\n","        accuracies_train.append(accr_train)\n","        accuracies_test.append(accr_test)\n","        \n","        print('Generation: ' + str(i+1) + '. 正答率(トレーニング) = ' + str(accr_train))\n","        print('                : ' + str(i+1) + '. 正答率(テスト) = ' + str(accr_test))               \n","        \n","lists = range(0, iters_num, plot_interval)\n","plt.plot(lists, accuracies_train, label=\"training set\")\n","plt.plot(lists, accuracies_test,  label=\"test set\")\n","plt.legend(loc=\"lower right\")\n","plt.title(\"accuracy\")\n","plt.xlabel(\"count\")\n","plt.ylabel(\"accuracy\")\n","plt.ylim(0, 1.0)\n","# グラフの表示\n","plt.show()"],"metadata":{"id":"VhuNo-6xVFQq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pickle\n","import numpy as np\n","from collections import OrderedDict\n","from common import layers\n","from common import optimizer\n","from data.mnist import load_mnist\n","import matplotlib.pyplot as plt\n","\n","# 画像データを２次元配列に変換\n","'''\n","input_data: 入力値\n","filter_h: フィルターの高さ\n","filter_w: フィルターの横幅\n","stride: ストライド\n","pad: パディング\n","'''\n","def im2col(input_data, filter_h, filter_w, stride=1, pad=0):\n","    # N: number, C: channel, H: height, W: width\n","    N, C, H, W = input_data.shape\n","    out_h = (H + 2 * pad - filter_h)//stride + 1\n","    out_w = (W + 2 * pad - filter_w)//stride + 1\n","\n","    img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')\n","    col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n","\n","    for y in range(filter_h):\n","        y_max = y + stride * out_h\n","        for x in range(filter_w):\n","            x_max = x + stride * out_w\n","            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n","    \n","    col = col.transpose(0, 4, 5, 1, 2, 3) # (N, C, filter_h, filter_w, out_h, out_w) -> (N, filter_w, out_h, out_w, C, filter_h)    \n","    \n","    col = col.reshape(N * out_h * out_w, -1)\n","    return col\n","\n","def col2im(col, input_shape, filter_h, filter_w, stride=1, pad=0):\n","    # N: number, C: channel, H: height, W: width\n","    N, C, H, W = input_shape\n","    # 切り捨て除算    \n","    out_h = (H + 2 * pad - filter_h)//stride + 1\n","    out_w = (W + 2 * pad - filter_w)//stride + 1\n","    col = col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2) # (N, filter_h, filter_w, out_h, out_w, C)\n","\n","    img = np.zeros((N, C, H + 2 * pad + stride - 1, W + 2 * pad + stride - 1))\n","    for y in range(filter_h):\n","        y_max = y + stride * out_h\n","        for x in range(filter_w):\n","            x_max = x + stride * out_w\n","            img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]\n","\n","    return img[:, :, pad:H + pad, pad:W + pad]\n","\n","class Convolution:\n","    # W: フィルター, b: バイアス\n","    def __init__(self, W, b, stride=1, pad=0):\n","        self.W = W\n","        self.b = b\n","        self.stride = stride\n","        self.pad = pad\n","        \n","        # 中間データ（backward時に使用）\n","        self.x = None   \n","        self.col = None\n","        self.col_W = None\n","        \n","        # フィルター・バイアスパラメータの勾配\n","        self.dW = None\n","        self.db = None\n","\n","    def forward(self, x):\n","        # FN: filter_number, C: channel, FH: filter_height, FW: filter_width\n","        FN, C, FH, FW = self.W.shape\n","        N, C, H, W = x.shape\n","        # 出力値のheight, width\n","        out_h = 1 + int((H + 2 * self.pad - FH) / self.stride)\n","        out_w = 1 + int((W + 2 * self.pad - FW) / self.stride)\n","        \n","        # xを行列に変換\n","        col = im2col(x, FH, FW, self.stride, self.pad)\n","        # フィルターをxに合わせた行列に変換\n","        col_W = self.W.reshape(FN, -1).T\n","\n","        out = np.dot(col, col_W) + self.b\n","        # 計算のために変えた形式を戻す\n","        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n","\n","        self.x = x\n","        self.col = col\n","        self.col_W = col_W\n","\n","        return out\n","\n","    def backward(self, dout):\n","        FN, C, FH, FW = self.W.shape\n","        dout = dout.transpose(0, 2, 3, 1).reshape(-1, FN)\n","\n","        self.db = np.sum(dout, axis=0)\n","        self.dW = np.dot(self.col.T, dout)\n","        self.dW = self.dW.transpose(1, 0).reshape(FN, C, FH, FW)\n","\n","        dcol = np.dot(dout, self.col_W.T)\n","        # dcolを画像データに変換\n","        dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)\n","\n","        return dx\n","\n","class Pooling:\n","    def __init__(self, pool_h, pool_w, stride=1, pad=0):\n","        self.pool_h = pool_h\n","        self.pool_w = pool_w\n","        self.stride = stride\n","        self.pad = pad\n","        \n","        self.x = None\n","        self.arg_max = None\n","\n","    def forward(self, x):\n","        N, C, H, W = x.shape\n","        out_h = int(1 + (H - self.pool_h) / self.stride)\n","        out_w = int(1 + (W - self.pool_w) / self.stride)\n","        \n","        # xを行列に変換\n","        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n","        # プーリングのサイズに合わせてリサイズ\n","        col = col.reshape(-1, self.pool_h*self.pool_w)\n","        \n","        # 行ごとに最大値を求める\n","        arg_max = np.argmax(col, axis=1)\n","        out = np.max(col, axis=1)\n","        # 整形\n","        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n","\n","        self.x = x\n","        self.arg_max = arg_max\n","\n","        return out\n","\n","    def backward(self, dout):\n","        dout = dout.transpose(0, 2, 3, 1)\n","        \n","        pool_size = self.pool_h * self.pool_w\n","        dmax = np.zeros((dout.size, pool_size))\n","        dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()\n","        dmax = dmax.reshape(dout.shape + (pool_size,)) \n","        \n","        dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)\n","        dx = col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)\n","        \n","        return dx\n","\n","class SimpleConvNet:\n","    # conv - relu - pool - affine - relu - affine - softmax\n","    def __init__(self, input_dim=(1, 28, 28), conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},\n","                 hidden_size=100, output_size=10, weight_init_std=0.01):\n","        filter_num = conv_param['filter_num']        \n","        filter_size = conv_param['filter_size']\n","        filter_pad = conv_param['pad']\n","        filter_stride = conv_param['stride']\n","        input_size = input_dim[1]\n","        conv_output_size = (input_size - filter_size + 2 * filter_pad) / filter_stride + 1\n","        pool_output_size = int(filter_num * (conv_output_size / 2) * (conv_output_size / 2))\n","\n","        # 重みの初期化\n","        self.params = {}\n","        self.params['W1'] = weight_init_std * np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n","        self.params['b1'] = np.zeros(filter_num)\n","        self.params['W2'] = weight_init_std * np.random.randn(pool_output_size, hidden_size)\n","        self.params['b2'] = np.zeros(hidden_size)\n","        self.params['W3'] = weight_init_std * np.random.randn(hidden_size, output_size)\n","        self.params['b3'] = np.zeros(output_size)\n","\n","        # レイヤの生成\n","        self.layers = OrderedDict()\n","        self.layers['Conv1'] = layers.Convolution(self.params['W1'], self.params['b1'], conv_param['stride'], conv_param['pad'])\n","        self.layers['Relu1'] = layers.Relu()\n","        self.layers['Pool1'] = layers.Pooling(pool_h=2, pool_w=2, stride=2)\n","        self.layers['Affine1'] = layers.Affine(self.params['W2'], self.params['b2'])\n","        self.layers['Relu2'] = layers.Relu()\n","        self.layers['Affine2'] = layers.Affine(self.params['W3'], self.params['b3'])\n","\n","        self.last_layer = layers.SoftmaxWithLoss()\n","\n","    def predict(self, x):\n","        for key in self.layers.keys():\n","            x = self.layers[key].forward(x)\n","        return x\n","        \n","    def loss(self, x, d):\n","        y = self.predict(x)\n","        return self.last_layer.forward(y, d)\n","\n","    def accuracy(self, x, d, batch_size=100):\n","        if d.ndim != 1 : d = np.argmax(d, axis=1)\n","        \n","        acc = 0.0\n","        \n","        for i in range(int(x.shape[0] / batch_size)):\n","            tx = x[i*batch_size:(i+1)*batch_size]\n","            td = d[i*batch_size:(i+1)*batch_size]\n","            y = self.predict(tx)\n","            y = np.argmax(y, axis=1)\n","            acc += np.sum(y == td) \n","        \n","        return acc / x.shape[0]\n","\n","    def gradient(self, x, d):\n","        # forward\n","        self.loss(x, d)\n","        \n","        # backward\n","        dout = 1\n","        dout = self.last_layer.backward(dout)\n","        layers = list(self.layers.values())\n","        \n","        layers.reverse()\n","        for layer in layers:\n","            dout = layer.backward(dout)\n","\n","        # 設定\n","        grad = {}\n","        grad['W1'], grad['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db\n","        grad['W2'], grad['b2'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n","        grad['W3'], grad['b3'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n","\n","        return grad\n","# im2colの処理確認\n","#input_data = np.random.rand(2, 1, 4, 4)*100//1 # number, channel, height, widthを表す\n","#print('========== input_data ===========\\n', input_data)\n","#print('==============================')\n","#filter_h = 3\n","#filter_w = 3\n","#stride = 1\n","#pad = 0\n","#col = im2col(input_data, filter_h=filter_h, filter_w=filter_w, stride=stride, pad=pad)\n","#print('============= col ==============\\n', col)\n","#print('==============================')\n","\n","\n","\n","from common import optimizer\n","\n","# データの読み込み\n","(x_train, d_train), (x_test, d_test) = load_mnist(flatten=False)\n","\n","print(\"データ読み込み完了\")\n","\n","# 処理に時間のかかる場合はデータを削減 \n","x_train, d_train = x_train[:5000], d_train[:5000]\n","x_test, d_test = x_test[:1000], d_test[:1000]\n","\n","\n","network = SimpleConvNet(input_dim=(1,28,28), conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n","                        hidden_size=100, output_size=10, weight_init_std=0.01)\n","\n","optimizer = optimizer.Adam()\n","\n","iters_num = 1000\n","train_size = x_train.shape[0]\n","batch_size = 100\n","\n","train_loss_list = []\n","accuracies_train = []\n","accuracies_test = []\n","\n","plot_interval=10\n","\n","\n","\n","for i in range(iters_num):\n","    batch_mask = np.random.choice(train_size, batch_size)\n","    x_batch = x_train[batch_mask]\n","    d_batch = d_train[batch_mask]\n","    \n","    grad = network.gradient(x_batch, d_batch)\n","    optimizer.update(network.params, grad)\n","\n","    loss = network.loss(x_batch, d_batch)\n","    train_loss_list.append(loss)\n","\n","    if (i+1) % plot_interval == 0:\n","        accr_train = network.accuracy(x_train, d_train)\n","        accr_test = network.accuracy(x_test, d_test)\n","        accuracies_train.append(accr_train)\n","        accuracies_test.append(accr_test)\n","        \n","        print('Generation: ' + str(i+1) + '. 正答率(トレーニング) = ' + str(accr_train))\n","        print('                : ' + str(i+1) + '. 正答率(テスト) = ' + str(accr_test))               \n","\n","lists = range(0, iters_num, plot_interval)\n","plt.plot(lists, accuracies_train, label=\"training set\")\n","plt.plot(lists, accuracies_test,  label=\"test set\")\n","plt.legend(loc=\"lower right\")\n","plt.title(\"accuracy\")\n","plt.xlabel(\"count\")\n","plt.ylabel(\"accuracy\")\n","plt.ylim(0, 1.0)\n","# グラフの表示\n","plt.show()\n","\n"],"metadata":{"id":"BbDoTXrsX9dD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pickle\n","import numpy as np\n","from collections import OrderedDict\n","from common import layers\n","from common import optimizer\n","from data.mnist import load_mnist\n","import matplotlib.pyplot as plt\n","\n","class DoubleConvNet:\n","    # conv - relu - pool - conv - relu - pool - affine - relu - affine - softmax\n","    def __init__(self, input_dim=(1, 28, 28),\n","                 conv_param_1={'filter_num':10, 'filter_size':7, 'pad':1, 'stride':1},\n","                 conv_param_2={'filter_num':20, 'filter_size':3, 'pad':1, 'stride':1},\n","                 hidden_size=100, output_size=10, weight_init_std=0.01):\n","        conv_output_size_1 = (input_dim[1] - conv_param_1['filter_size'] + 2 * conv_param_1['pad']) / conv_param_1['stride'] + 1\n","        conv_output_size_2 = (conv_output_size_1 / 2 - conv_param_2['filter_size'] + 2 * conv_param_2['pad']) / conv_param_2['stride'] + 1        \n","        pool_output_size = int(conv_param_2['filter_num'] * (conv_output_size_2 / 2) * (conv_output_size_2 / 2))        \n","        # 重みの初期化\n","        self.params = {}\n","        self.params['W1'] = weight_init_std * np.random.randn(conv_param_1['filter_num'], input_dim[0], conv_param_1['filter_size'], conv_param_1['filter_size'])\n","        self.params['b1'] = np.zeros(conv_param_1['filter_num'])\n","        self.params['W2'] = weight_init_std * np.random.randn(conv_param_2['filter_num'], conv_param_1['filter_num'], conv_param_2['filter_size'], conv_param_2['filter_size'])\n","        self.params['b2'] = np.zeros(conv_param_2['filter_num'])\n","        self.params['W3'] = weight_init_std * np.random.randn(pool_output_size, hidden_size)\n","        self.params['b3'] = np.zeros(hidden_size)\n","        self.params['W4'] = weight_init_std * np.random.randn(hidden_size, output_size)\n","        self.params['b4'] = np.zeros(output_size)\n","        # レイヤの生成\n","        self.layers = OrderedDict()\n","        self.layers['Conv1'] = layers.Convolution(self.params['W1'], self.params['b1'], conv_param_1['stride'], conv_param_1['pad'])\n","        self.layers['Relu1'] = layers.Relu()\n","        self.layers['Pool1'] = layers.Pooling(pool_h=2, pool_w=2, stride=2)\n","        self.layers['Conv2'] = layers.Convolution(self.params['W2'], self.params['b2'], conv_param_2['stride'], conv_param_2['pad'])\n","        self.layers['Relu2'] = layers.Relu()\n","        self.layers['Pool2'] = layers.Pooling(pool_h=2, pool_w=2, stride=2)\n","        self.layers['Affine1'] = layers.Affine(self.params['W3'], self.params['b3'])\n","        self.layers['Relu3'] = layers.Relu()\n","        self.layers['Affine2'] = layers.Affine(self.params['W4'], self.params['b4'])\n","        self.last_layer = layers.SoftmaxWithLoss()\n","\n","    def predict(self, x):\n","        for key in self.layers.keys():\n","            x = self.layers[key].forward(x)\n","        return x\n","        \n","    def loss(self, x, d):\n","        y = self.predict(x)\n","        return self.last_layer.forward(y, d)\n","\n","    def accuracy(self, x, d, batch_size=100):\n","        if d.ndim != 1 : d = np.argmax(d, axis=1)\n","        \n","        acc = 0.0\n","        \n","        for i in range(int(x.shape[0] / batch_size)):\n","            tx = x[i*batch_size:(i+1)*batch_size]\n","            td = d[i*batch_size:(i+1)*batch_size]\n","            y = self.predict(tx)\n","            y = np.argmax(y, axis=1)\n","            acc += np.sum(y == td) \n","        \n","        return acc / x.shape[0]\n","\n","    def gradient(self, x, d):\n","        # forward\n","        self.loss(x, d)\n","        \n","        # backward\n","        dout = 1\n","        dout = self.last_layer.backward(dout)\n","        layers = list(self.layers.values())\n","        \n","        layers.reverse()\n","        for layer in layers:\n","            dout = layer.backward(dout)\n","\n","        # 設定\n","        grad = {}\n","        grad['W1'], grad['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db\n","        grad['W2'], grad['b2'] = self.layers['Conv2'].dW, self.layers['Conv2'].db        \n","        grad['W3'], grad['b3'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n","        grad['W4'], grad['b4'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n","\n","        return grad\n","\n","# データの読み込み\n","(x_train, d_train), (x_test, d_test) = load_mnist(flatten=False)\n","\n","print(\"データ読み込み完了\")\n","# 処理に時間のかかる場合はデータを削減 \n","x_train, d_train = x_train[:5000], d_train[:5000]\n","x_test, d_test = x_test[:1000], d_test[:1000]\n","\n","\n","network = DoubleConvNet(input_dim=(1,28,28), \n","                          conv_param_1={'filter_num':10, 'filter_size':7, 'pad':1, 'stride':1},\n","                          conv_param_2={'filter_num':20, 'filter_size':3, 'pad':1, 'stride':1},\n","                          hidden_size=100, output_size=10, weight_init_std=0.01)\n","\n","optimizer = optimizer.Adam()\n","\n","# 時間がかかるため100に設定\n","iters_num = 100\n","# iters_num = 1000\n","train_size = x_train.shape[0]\n","batch_size = 100\n","\n","train_loss_list = []\n","accuracies_train = []\n","accuracies_test = []\n","\n","plot_interval=10\n","\n","\n","\n","for i in range(iters_num):\n","    batch_mask = np.random.choice(train_size, batch_size)\n","    x_batch = x_train[batch_mask]\n","    d_batch = d_train[batch_mask]\n","    \n","    grad = network.gradient(x_batch, d_batch)\n","    optimizer.update(network.params, grad)\n","    loss = network.loss(x_batch, d_batch)\n","    train_loss_list.append(loss)\n","\n","    if (i+1) % plot_interval == 0:\n","        accr_train = network.accuracy(x_train, d_train)\n","        accr_test = network.accuracy(x_test, d_test)\n","        accuracies_train.append(accr_train)\n","        accuracies_test.append(accr_test)\n","        \n","        print('Generation: ' + str(i+1) + '. 正答率(トレーニング) = ' + str(accr_train))\n","        print('                : ' + str(i+1) + '. 正答率(テスト) = ' + str(accr_test))               \n","\n","lists = range(0, iters_num, plot_interval)\n","plt.plot(lists, accuracies_train, label=\"training set\")\n","plt.plot(lists, accuracies_test,  label=\"test set\")\n","plt.legend(loc=\"lower right\")\n","plt.title(\"accuracy\")\n","plt.xlabel(\"count\")\n","plt.ylabel(\"accuracy\")\n","plt.ylim(0, 1.0)\n","# グラフの表示\n","plt.show()"],"metadata":{"id":"_4Fy-GgLZQPk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pickle\n","import numpy as np\n","from collections import OrderedDict\n","from common import layers\n","from data.mnist import load_mnist\n","import matplotlib.pyplot as plt\n","from common import optimizer\n","\n","class DeepConvNet:\n","    '''\n","    認識率99%以上の高精度なConvNet\n","\n","    conv - relu - conv- relu - pool -\n","    conv - relu - conv- relu - pool -\n","    conv - relu - conv- relu - pool -\n","    affine - relu - dropout - affine - dropout - softmax\n","    '''\n","    def __init__(self, input_dim=(1, 28, 28),\n","                 conv_param_1 = {'filter_num':16, 'filter_size':3, 'pad':1, 'stride':1},\n","                 conv_param_2 = {'filter_num':16, 'filter_size':3, 'pad':1, 'stride':1},\n","                 conv_param_3 = {'filter_num':32, 'filter_size':3, 'pad':1, 'stride':1},\n","                 conv_param_4 = {'filter_num':32, 'filter_size':3, 'pad':2, 'stride':1},\n","                 conv_param_5 = {'filter_num':64, 'filter_size':3, 'pad':1, 'stride':1},\n","                 conv_param_6 = {'filter_num':64, 'filter_size':3, 'pad':1, 'stride':1},\n","                 hidden_size=50, output_size=10):\n","        # 重みの初期化===========\n","        # 各層のニューロンひとつあたりが、前層のニューロンといくつのつながりがあるか\n","        pre_node_nums = np.array([1*3*3, 16*3*3, 16*3*3, 32*3*3, 32*3*3, 64*3*3, 64*4*4, hidden_size])\n","        wight_init_scales = np.sqrt(2.0 / pre_node_nums)  # Heの初期値\n","        \n","        self.params = {}\n","        pre_channel_num = input_dim[0]\n","        for idx, conv_param in enumerate([conv_param_1, conv_param_2, conv_param_3, conv_param_4, conv_param_5, conv_param_6]):\n","            self.params['W' + str(idx+1)] = wight_init_scales[idx] * np.random.randn(conv_param['filter_num'], pre_channel_num, conv_param['filter_size'], conv_param['filter_size'])\n","            self.params['b' + str(idx+1)] = np.zeros(conv_param['filter_num'])\n","            pre_channel_num = conv_param['filter_num']\n","        self.params['W7'] = wight_init_scales[6] * np.random.randn(pre_node_nums[6], hidden_size)\n","        print(self.params['W7'].shape)\n","        self.params['b7'] = np.zeros(hidden_size)\n","        self.params['W8'] = wight_init_scales[7] * np.random.randn(pre_node_nums[7], output_size)\n","        self.params['b8'] = np.zeros(output_size)\n","\n","        # レイヤの生成===========\n","        self.layers = []\n","        self.layers.append(layers.Convolution(self.params['W1'], self.params['b1'], \n","                           conv_param_1['stride'], conv_param_1['pad']))\n","        self.layers.append(layers.Relu())\n","        self.layers.append(layers.Convolution(self.params['W2'], self.params['b2'], \n","                           conv_param_2['stride'], conv_param_2['pad']))\n","        self.layers.append(layers.Relu())\n","        self.layers.append(layers.Pooling(pool_h=2, pool_w=2, stride=2))\n","        self.layers.append(layers.Convolution(self.params['W3'], self.params['b3'], \n","                           conv_param_3['stride'], conv_param_3['pad']))\n","        self.layers.append(layers.Relu())\n","        self.layers.append(layers.Convolution(self.params['W4'], self.params['b4'],\n","                           conv_param_4['stride'], conv_param_4['pad']))\n","        self.layers.append(layers.Relu())\n","        self.layers.append(layers.Pooling(pool_h=2, pool_w=2, stride=2))\n","        self.layers.append(layers.Convolution(self.params['W5'], self.params['b5'],\n","                           conv_param_5['stride'], conv_param_5['pad']))\n","        self.layers.append(layers.Relu())\n","        self.layers.append(layers.Convolution(self.params['W6'], self.params['b6'],\n","                           conv_param_6['stride'], conv_param_6['pad']))\n","        self.layers.append(layers.Relu())\n","        self.layers.append(layers.Pooling(pool_h=2, pool_w=2, stride=2))\n","        self.layers.append(layers.Affine(self.params['W7'], self.params['b7']))\n","        self.layers.append(layers.Relu())\n","        self.layers.append(layers.Dropout(0.5))\n","        self.layers.append(layers.Affine(self.params['W8'], self.params['b8']))\n","        self.layers.append(layers.Dropout(0.5))\n","        \n","        self.last_layer = layers.SoftmaxWithLoss()\n","\n","    def predict(self, x, train_flg=False):\n","        for layer in self.layers:\n","            if isinstance(layer, layers.Dropout):\n","                x = layer.forward(x, train_flg)\n","            else:\n","                x = layer.forward(x)\n","        return x\n","\n","    def loss(self, x, d):\n","        y = self.predict(x, train_flg=True)\n","        return self.last_layer.forward(y, d)\n","\n","    def accuracy(self, x, d, batch_size=100):\n","        if d.ndim != 1 : d = np.argmax(d, axis=1)\n","\n","        acc = 0.0\n","\n","        for i in range(int(x.shape[0] / batch_size)):\n","            tx = x[i*batch_size:(i+1)*batch_size]\n","            td = d[i*batch_size:(i+1)*batch_size]\n","            y = self.predict(tx, train_flg=False)\n","            y = np.argmax(y, axis=1)\n","            acc += np.sum(y == td)\n","\n","        return acc / x.shape[0]\n","\n","    def gradient(self, x, d):\n","        # forward\n","        self.loss(x, d)\n","\n","        # backward\n","        dout = 1\n","        dout = self.last_layer.backward(dout)\n","\n","        tmp_layers = self.layers.copy()\n","        tmp_layers.reverse()\n","        for layer in tmp_layers:\n","            dout = layer.backward(dout)\n","\n","        # 設定\n","        grads = {}\n","        for i, layer_idx in enumerate((0, 2, 5, 7, 10, 12, 15, 18)):\n","            grads['W' + str(i+1)] = self.layers[layer_idx].dW\n","            grads['b' + str(i+1)] = self.layers[layer_idx].db\n","\n","        return grads\n","\n","(x_train, d_train), (x_test, d_test) = load_mnist(flatten=False)\n","\n","# 処理に時間のかかる場合はデータを削減 \n","x_train, d_train = x_train[:5000], d_train[:5000]\n","x_test, d_test = x_test[:1000], d_test[:1000]\n","\n","print(\"データ読み込み完了\")\n","\n","network = DeepConvNet()  \n","optimizer = optimizer.Adam()\n","\n","iters_num = 1000\n","train_size = x_train.shape[0]\n","batch_size = 100\n","\n","train_loss_list = []\n","accuracies_train = []\n","accuracies_test = []\n","\n","plot_interval=10\n","\n","\n","for i in range(iters_num):\n","    batch_mask = np.random.choice(train_size, batch_size)\n","    x_batch = x_train[batch_mask]\n","    d_batch = d_train[batch_mask]\n","    \n","    grad = network.gradient(x_batch, d_batch)\n","    optimizer.update(network.params, grad)\n","\n","    loss = network.loss(x_batch, d_batch)\n","    train_loss_list.append(loss)\n","\n","    if (i+1) % plot_interval == 0:\n","        accr_train = network.accuracy(x_train, d_train)\n","        accr_test = network.accuracy(x_test, d_test)\n","        accuracies_train.append(accr_train)\n","        accuracies_test.append(accr_test)\n","        \n","        print('Generation: ' + str(i+1) + '. 正答率(トレーニング) = ' + str(accr_train))\n","        print('                : ' + str(i+1) + '. 正答率(テスト) = ' + str(accr_test))               \n","\n","lists = range(0, iters_num, plot_interval)\n","plt.plot(lists, accuracies_train, label=\"training set\")\n","plt.plot(lists, accuracies_test,  label=\"test set\")\n","plt.legend(loc=\"lower right\")\n","plt.title(\"accuracy\")\n","plt.xlabel(\"count\")\n","plt.ylabel(\"accuracy\")\n","plt.ylim(0, 1.0)\n","# グラフの表示\n","plt.show()"],"metadata":{"id":"M9MrwHFQZnfF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["確認テストの結果と考察\n","\n","連鎖律の原理を使い、dz/dxを求めよ。（2分）z = t2t = x +y10\n","＝合格：連鎖律が誤差逆伝播の鍵になっている\n","\n","シグモイド関数を微分した時、入力値が0の時に最大値をとる。その値として正しいものを選択肢から選べ。\n","＝０．２５：合格：１より小さいことが重要\n","\n","重みの初期値に0を設定すると、どのような問題が発生するか。簡潔に説明せよ。\n","＝学習がすすまない：合格\n","\n","一般的に考えられるバッチ正規化の効果を2点挙げよ。\n","＝\n","過学習を抑制、初期値の設定に過敏にならなくてすむ、学習時間の短縮\n","\n","例題チャレンジ＝合格\n","\n","モメンタム・AdaGrad・RMSPropの特徴をそれぞれ簡潔に説明せよ。\n","＝\n","モメンタム＝一期前の勾配情報を用いることで振動を抑制でき収束が安定\n","\n","Adagrad＝今までの更新量が大きい次元ほど更新されにくく、今までの更新量が小さい次元ほどその方向に更新されやすくなる。\n","\n","RMSProp=Adagradでは最後に更新がとまってしまうため、最近の勾配ほど強く（昔の勾配は弱く）影響をうけるように調整した。\n","\n","線形モデルの正則化手法、リッジ回帰の特徴として正しいものを選択しなさい。\n","＝合格：ラッソ（L1）とリッジ（L2）の違いに注意すること\n","\n","下図について、L1正則化を表しているグラフはどちらか答えよ。\n","＝合格\n","\n","以下はL2正則化を適用した場合にパラメータの更新を行うプログラムである。あるパラメータparamと正則化がない時にパラメータに伝播される誤差の勾配gradが与えられたとする。 最終的な勾配を計算する(え)に当てはまるのはどれか。ただし、rateはL2正則化の係数を表すとする。\n","＝合格\n","＊誤差逆伝播する際には微分していることに注意\n","\n","以下はL1正則化(Lasso)を適用した場合に、パラメータの更新を行うプログラムである。あるパラメータparamと正則化がない時にパラメータに伝播される誤差の勾配gradが与えられたとする。 最終的な勾配を計算する(お)に当てはまるのはどれか。ただし、rateはL1正則化の係数を表すとする。\n","＝合格\n","＊numpyのsignは符号を表す関数である\n","\n","以下は画像をランダムに切り取る処理を行うプログラムである。これは画像中の物体の位置を移動させるなどの意味がある。(か)に当てはまるのはどれか。\n","＝合格\n","\n","サイズ6×6の入力画像を、サイズ2×2のフィルタで畳み込んだ時の出力画像のサイズを答えよ。なおストライドとパディングは1とする。\n","＝合格\n","（サイズ＋２＊パディングーフィルタサイズ）/2 +1で計算すること。"],"metadata":{"id":"VqRyeLeJhJ2O"}}]}