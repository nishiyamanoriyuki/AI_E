{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.3"},"colab":{"name":"deep_learning_day3.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true}},"cells":[{"cell_type":"markdown","metadata":{"id":"YkwjN1jNVAYy"},"source":["## 要点とまとめ"]},{"cell_type":"markdown","source":["section1:再帰型ニューラルネットワークの概念\n","RNNとは時系列データに対応可能な、ニューラルネットワークである。\n","初期の状態と過去の時間t-1の状態を保持し、そこから次の時間でのtを再帰的に求める再帰構造をもつ。\n","BPTTとはRNNにおける誤差逆伝播法の一種である。BPTTでは計算結果（=誤差）から微分を逆算することで、不要な再帰的計算を避けて微分を算出できる。\n","\n","section２:LSTM\n","RNNは時間を遡るほど勾配が消失していくのでLSTMはそれを解決する。\n","LSTMの特徴として、CEC、入力ゲートと出力ゲート、忘却ゲート、覗き穴結合がある。\n","CECは勾配消失または爆発問題に対して勾配を１にすることで解決した。\n","覗き穴結合とはCEC自身の値に、重み行列を介して伝播可能にした構造である。\n","\n","section３:GRU\n","LSTMは、パラメータが多数存在していたため、計算負荷が大きい。\n","GRUは、パラメータを大幅に削減し、精度は同等またはそれ以上が望める様になった構造。\n","GRUは「リセットゲート」と入力ゲートと出力ゲートをあわせた「更新ゲート」を持つ。\n","\n","section４:双方向RNN\n","双方向RNNとは過去の情報だけでなく、未来の情報を加味することで、精度を向上させるためのモデルである。\n","双方向RNNの注意点は、未来の情報が分かっていなければ使えない。\n","文章の推敲や、機械翻訳、フレーム間の補完などのタスクに使用する。\n","\n","section５:seq2seq\n","Seq2seqはEncoder-Decoderモデルの一種で機械翻訳や機械対話に用いられる。\n","\n","Encoder\n","Takingは文章を単語等のトークン毎に分割し、トークンごとのIDに分割する。\n","EmbeddingはIDから、そのトークンを表す分散表現ベクトルに変換する。\n","Encoder RNNではベクトルを順番にRNNに入力していく。\n","\n","Decoder\n","Decoder RNNでは Encoder RNN のfinal stateからEmbedding を入力\n","生成確率にもとづいてtoken をランダムにSamplingする\n","token をEmbedding してDecoder RNN への次の入力にする\n","上記を繰り返してtoken を文字列になおす（Detokenize）\n","\n","HRED\n","Seq2Seq+ Context RNNdで過去の発話の履歴を加味した返答をできる。\n","Context RNNはEncoder のまとめた各文章の系列をまとめて、これまでの会話コンテキスト全体を表すベクトルに変換する。\n","\n","VHRED\n","VHREDはHREDに、VAEの潜在変数の概念を追加した。\n","VAEは潜在変数zに確率分布z∼N(0,1)を仮定したものである。\n","\n","section6:word2vec\n","word2vecはone_hotベクトル表現からembedding表現を得るためのもの。\n","CBOW(continuous bag-of-words)とskip-gramが使われ、skip-gramの方が精度が高いが学習コストも大きい。\n","CBOWモデルはコンテクストからターゲットを推測し、skip-gramは中央の単語から前後の複数のコンテクストを予測する。\n","\n","\n","section7:attention mechanism\n","attention mechanismは入力と出力のどの単語が関連しているか学習する。\n","Seq2seqが中間層が固定次元ベクトルで表されるため長い文章への対応が難しいため、attention mechanismが使われる。\n","単語間の予測関連度を，アテンション係数として推定する.\n","スコア関数は２つのベクトル間の距離尺度のような関連度(relevance)を学習させる."],"metadata":{"id":"cBzbHaKuQtxi"}},{"cell_type":"markdown","source":["## 実装演習"],"metadata":{"id":"SW9X3qL_RDUS"}},{"cell_type":"code","metadata":{"id":"pvFXpiH3EVC1"},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","import sys\n","sys.path.append('/content/drive/My Drive/DNN_code')\n","import os\n","os.chdir('drive/My Drive/DNN_code/lesson_3/3_2_tf_languagemodel/')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"KNSG0aKXO-bk"},"source":["import numpy as np\n","from common import functions\n","import matplotlib.pyplot as plt\n","\n","\n","def d_tanh(x):\n","    return 1/(np.cosh(x) ** 2)\n","\n","# データを用意\n","# 2進数の桁数\n","binary_dim = 8\n","# 最大値 + 1\n","largest_number = pow(2, binary_dim)\n","# largest_numberまで2進数を用意\n","binary = np.unpackbits(np.array([range(largest_number)],dtype=np.uint8).T,axis=1)\n","\n","input_layer_size = 2\n","hidden_layer_size = 16\n","output_layer_size = 1\n","\n","weight_init_std = 1\n","learning_rate = 0.1\n","\n","iters_num = 10000\n","plot_interval = 100\n","\n","# ウェイト初期化 (バイアスは簡単のため省略)\n","W_in = weight_init_std * np.random.randn(input_layer_size, hidden_layer_size)\n","W_out = weight_init_std * np.random.randn(hidden_layer_size, output_layer_size)\n","W = weight_init_std * np.random.randn(hidden_layer_size, hidden_layer_size)\n","# Xavier\n","# W_in = np.random.randn(input_layer_size, hidden_layer_size) / (np.sqrt(input_layer_size))\n","# W_out = np.random.randn(hidden_layer_size, output_layer_size) / (np.sqrt(hidden_layer_size))\n","# W = np.random.randn(hidden_layer_size, hidden_layer_size) / (np.sqrt(hidden_layer_size))\n","# He\n","# W_in = np.random.randn(input_layer_size, hidden_layer_size) / (np.sqrt(input_layer_size)) * np.sqrt(2)\n","# W_out = np.random.randn(hidden_layer_size, output_layer_size) / (np.sqrt(hidden_layer_size)) * np.sqrt(2)\n","# W = np.random.randn(hidden_layer_size, hidden_layer_size) / (np.sqrt(hidden_layer_size)) * np.sqrt(2)\n","\n","\n","# 勾配\n","W_in_grad = np.zeros_like(W_in)\n","W_out_grad = np.zeros_like(W_out)\n","W_grad = np.zeros_like(W)\n","\n","u = np.zeros((hidden_layer_size, binary_dim + 1))\n","z = np.zeros((hidden_layer_size, binary_dim + 1))\n","y = np.zeros((output_layer_size, binary_dim))\n","\n","delta_out = np.zeros((output_layer_size, binary_dim))\n","delta = np.zeros((hidden_layer_size, binary_dim + 1))\n","\n","all_losses = []\n","\n","for i in range(iters_num):\n","    \n","    # A, B初期化 (a + b = d)\n","    a_int = np.random.randint(largest_number/2)\n","    a_bin = binary[a_int] # binary encoding\n","    b_int = np.random.randint(largest_number/2)\n","    b_bin = binary[b_int] # binary encoding\n","    \n","    # 正解データ\n","    d_int = a_int + b_int\n","    d_bin = binary[d_int]\n","    \n","    # 出力バイナリ\n","    out_bin = np.zeros_like(d_bin)\n","    \n","    # 時系列全体の誤差\n","    all_loss = 0    \n","    \n","    # 時系列ループ\n","    for t in range(binary_dim):\n","        # 入力値\n","        X = np.array([a_bin[ - t - 1], b_bin[ - t - 1]]).reshape(1, -1)\n","        # 時刻tにおける正解データ\n","        dd = np.array([d_bin[binary_dim - t - 1]])\n","        \n","        u[:,t+1] = np.dot(X, W_in) + np.dot(z[:,t].reshape(1, -1), W)\n","        z[:,t+1] = functions.sigmoid(u[:,t+1])\n","#         z[:,t+1] = functions.relu(u[:,t+1])\n","#         z[:,t+1] = np.tanh(u[:,t+1])    \n","        y[:,t] = functions.sigmoid(np.dot(z[:,t+1].reshape(1, -1), W_out))\n","\n","\n","        #誤差\n","        loss = functions.mean_squared_error(dd, y[:,t])\n","        \n","        delta_out[:,t] = functions.d_mean_squared_error(dd, y[:,t]) * functions.d_sigmoid(y[:,t])        \n","        \n","        all_loss += loss\n","\n","        out_bin[binary_dim - t - 1] = np.round(y[:,t])\n","    \n","    \n","    for t in range(binary_dim)[::-1]:\n","        X = np.array([a_bin[-t-1],b_bin[-t-1]]).reshape(1, -1)        \n","\n","        delta[:,t] = (np.dot(delta[:,t+1].T, W.T) + np.dot(delta_out[:,t].T, W_out.T)) * functions.d_sigmoid(u[:,t+1])\n","#         delta[:,t] = (np.dot(delta[:,t+1].T, W.T) + np.dot(delta_out[:,t].T, W_out.T)) * functions.d_relu(u[:,t+1])\n","#         delta[:,t] = (np.dot(delta[:,t+1].T, W.T) + np.dot(delta_out[:,t].T, W_out.T)) * d_tanh(u[:,t+1])    \n","\n","        # 勾配更新\n","        W_out_grad += np.dot(z[:,t+1].reshape(-1,1), delta_out[:,t].reshape(-1,1))\n","        W_grad += np.dot(z[:,t].reshape(-1,1), delta[:,t].reshape(1,-1))\n","        W_in_grad += np.dot(X.T, delta[:,t].reshape(1,-1))\n","    \n","    # 勾配適用\n","    W_in -= learning_rate * W_in_grad\n","    W_out -= learning_rate * W_out_grad\n","    W -= learning_rate * W_grad\n","    \n","    W_in_grad *= 0\n","    W_out_grad *= 0\n","    W_grad *= 0\n","    \n","\n","    if(i % plot_interval == 0):\n","        all_losses.append(all_loss)        \n","        print(\"iters:\" + str(i))\n","        print(\"Loss:\" + str(all_loss))\n","        print(\"Pred:\" + str(out_bin))\n","        print(\"True:\" + str(d_bin))\n","        out_int = 0\n","        for index,x in enumerate(reversed(out_bin)):\n","            out_int += x * pow(2, index)\n","        print(str(a_int) + \" + \" + str(b_int) + \" = \" + str(out_int))\n","        print(\"------------\")\n","\n","lists = range(0, iters_num, plot_interval)\n","plt.plot(lists, all_losses, label=\"loss\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","import numpy as np\n","import re\n","import glob\n","import collections\n","import random\n","import pickle\n","import time\n","import datetime\n","import os\n","\n","# logging levelを変更\n","tf.logging.set_verbosity(tf.logging.ERROR)\n","\n","class Corpus:\n","    def __init__(self):\n","        self.unknown_word_symbol = \"<???>\" # 出現回数の少ない単語は未知語として定義しておく\n","        self.unknown_word_threshold = 3 # 未知語と定義する単語の出現回数の閾値\n","        self.corpus_file = \"./corpus/**/*.txt\"\n","        self.corpus_encoding = \"utf-8\"\n","        self.dictionary_filename = \"./data_for_predict/word_dict.dic\"\n","        self.chunk_size = 5\n","        self.load_dict()\n","\n","        words = []\n","        for filename in glob.glob(self.corpus_file, recursive=True):\n","            with open(filename, \"r\", encoding=self.corpus_encoding) as f:\n","\n","                # word breaking\n","                text = f.read()\n","                # 全ての文字を小文字に統一し、改行をスペースに変換\n","                text = text.lower().replace(\"\\n\", \" \")\n","                # 特定の文字以外の文字を空文字に置換する\n","                text = re.sub(r\"[^a-z '\\-]\", \"\", text)\n","                # 複数のスペースはスペース一文字に変換\n","                text = re.sub(r\"[ ]+\", \" \", text)\n","\n","                # 前処理： '-' で始まる単語は無視する\n","                words = [ word for word in text.split() if not word.startswith(\"-\")]\n","\n","\n","        self.data_n = len(words) - self.chunk_size\n","        self.data = self.seq_to_matrix(words)\n","\n","    def prepare_data(self):\n","        \"\"\"\n","        訓練データとテストデータを準備する。\n","        data_n = ( text データの総単語数 ) - chunk_size\n","        input: (data_n, chunk_size, vocabulary_size)\n","        output: (data_n, vocabulary_size)\n","        \"\"\"\n","\n","        # 入力と出力の次元テンソルを準備\n","        all_input = np.zeros([self.chunk_size, self.vocabulary_size, self.data_n])\n","        all_output = np.zeros([self.vocabulary_size, self.data_n])\n","\n","        # 準備したテンソルに、コーパスの one-hot 表現(self.data) のデータを埋めていく\n","        # i 番目から ( i + chunk_size - 1 ) 番目までの単語が１組の入力となる\n","        # このときの出力は ( i + chunk_size ) 番目の単語\n","        for i in range(self.data_n):\n","            all_output[:, i] = self.data[:, i + self.chunk_size] # (i + chunk_size) 番目の単語の one-hot ベクトル\n","            for j in range(self.chunk_size):\n","                all_input[j, :, i] = self.data[:, i + self.chunk_size - j - 1]\n","\n","        # 後に使うデータ形式に合わせるために転置を取る\n","        all_input = all_input.transpose([2, 0, 1])\n","        all_output = all_output.transpose()\n","\n","        # 訓練データ：テストデータを 4 : 1 に分割する\n","        training_num = ( self.data_n * 4 ) // 5\n","        return all_input[:training_num], all_output[:training_num], all_input[training_num:], all_output[training_num:]\n","\n","\n","    def build_dict(self):\n","        # コーパス全体を見て、単語の出現回数をカウントする\n","        counter = collections.Counter()\n","        for filename in glob.glob(self.corpus_file, recursive=True):\n","            with open(filename, \"r\", encoding=self.corpus_encoding) as f:\n","\n","                # word breaking\n","                text = f.read()\n","                # 全ての文字を小文字に統一し、改行をスペースに変換\n","                text = text.lower().replace(\"\\n\", \" \")\n","                # 特定の文字以外の文字を空文字に置換する\n","                text = re.sub(r\"[^a-z '\\-]\", \"\", text)\n","                # 複数のスペースはスペース一文字に変換\n","                text = re.sub(r\"[ ]+\", \" \", text)\n","\n","                # 前処理： '-' で始まる単語は無視する\n","                words = [word for word in text.split() if not word.startswith(\"-\")]\n","\n","                counter.update(words)\n","\n","        # 出現頻度の低い単語を一つの記号にまとめる\n","        word_id = 0\n","        dictionary = {}\n","        for word, count in counter.items():\n","            if count <= self.unknown_word_threshold:\n","                continue\n","\n","            dictionary[word] = word_id\n","            word_id += 1\n","        dictionary[self.unknown_word_symbol] = word_id\n","\n","        print(\"総単語数：\", len(dictionary))\n","\n","        # 辞書を pickle を使って保存しておく\n","        with open(self.dictionary_filename, \"wb\") as f:\n","            pickle.dump(dictionary, f)\n","            print(\"Dictionary is saved to\", self.dictionary_filename)\n","\n","        self.dictionary = dictionary\n","\n","        print(self.dictionary)\n","\n","    def load_dict(self):\n","        with open(self.dictionary_filename, \"rb\") as f:\n","            self.dictionary = pickle.load(f)\n","            self.vocabulary_size = len(self.dictionary)\n","            self.input_layer_size = len(self.dictionary)\n","            self.output_layer_size = len(self.dictionary)\n","            print(\"総単語数: \", self.input_layer_size)\n","\n","    def get_word_id(self, word):\n","        # print(word)\n","        # print(self.dictionary)\n","        # print(self.unknown_word_symbol)\n","        # print(self.dictionary[self.unknown_word_symbol])\n","        # print(self.dictionary.get(word, self.dictionary[self.unknown_word_symbol]))\n","        return self.dictionary.get(word, self.dictionary[self.unknown_word_symbol])\n","\n","    # 入力された単語を one-hot ベクトルにする\n","    def to_one_hot(self, word):\n","        index = self.get_word_id(word)\n","        data = np.zeros(self.vocabulary_size)\n","        data[index] = 1\n","        return data\n","\n","    def seq_to_matrix(self, seq):\n","        print(seq)\n","        data = np.array([self.to_one_hot(word) for word in seq]) # (data_n, vocabulary_size)\n","        return data.transpose() # (vocabulary_size, data_n)\n","\n","class Language:\n","    \"\"\"\n","    input layer: self.vocabulary_size\n","    hidden layer: rnn_size = 30\n","    output layer: self.vocabulary_size\n","    \"\"\"\n","\n","    def __init__(self):\n","        self.corpus = Corpus()\n","        self.dictionary = self.corpus.dictionary\n","        self.vocabulary_size = len(self.dictionary) # 単語数\n","        self.input_layer_size = self.vocabulary_size # 入力層の数\n","        self.hidden_layer_size = 30 # 隠れ層の RNN ユニットの数\n","        self.output_layer_size = self.vocabulary_size # 出力層の数\n","        self.batch_size = 128 # バッチサイズ\n","        self.chunk_size = 5 # 展開するシーケンスの数。c_0, c_1, ..., c_(chunk_size - 1) を入力し、c_(chunk_size) 番目の単語の確率が出力される。\n","        self.learning_rate = 0.005 # 学習率\n","        self.epochs = 1000 # 学習するエポック数\n","        self.forget_bias = 1.0 # LSTM における忘却ゲートのバイアス\n","        self.model_filename = \"./data_for_predict/predict_model.ckpt\"\n","        self.unknown_word_symbol = self.corpus.unknown_word_symbol\n","\n","    def inference(self, input_data, initial_state):\n","        \"\"\"\n","        :param input_data: (batch_size, chunk_size, vocabulary_size) 次元のテンソル\n","        :param initial_state: (batch_size, hidden_layer_size) 次元の行列\n","        :return:\n","        \"\"\"\n","        # 重みとバイアスの初期化\n","        hidden_w = tf.Variable(tf.truncated_normal([self.input_layer_size, self.hidden_layer_size], stddev=0.01))\n","        hidden_b = tf.Variable(tf.ones([self.hidden_layer_size]))\n","        output_w = tf.Variable(tf.truncated_normal([self.hidden_layer_size, self.output_layer_size], stddev=0.01))\n","        output_b = tf.Variable(tf.ones([self.output_layer_size]))\n","\n","        # BasicLSTMCell, BasicRNNCell は (batch_size, hidden_layer_size) が chunk_size 数ぶんつながったリストを入力とする。\n","        # 現時点での入力データは (batch_size, chunk_size, input_layer_size) という３次元のテンソルなので\n","        # tf.transpose や tf.reshape などを駆使してテンソルのサイズを調整する。\n","\n","        input_data = tf.transpose(input_data, [1, 0, 2]) # 転置。(chunk_size, batch_size, vocabulary_size)\n","        input_data = tf.reshape(input_data, [-1, self.input_layer_size]) # 変形。(chunk_size * batch_size, input_layer_size)\n","        input_data = tf.matmul(input_data, hidden_w) + hidden_b # 重みWとバイアスBを適用。 (chunk_size, batch_size, hidden_layer_size)\n","        input_data = tf.split(input_data, self.chunk_size, 0) # リストに分割。chunk_size * (batch_size, hidden_layer_size)\n","\n","        # RNN のセルを定義する。RNN Cell の他に LSTM のセルや GRU のセルなどが利用できる。\n","        cell = tf.nn.rnn_cell.BasicRNNCell(self.hidden_layer_size)\n","        outputs, states = tf.nn.static_rnn(cell, input_data, initial_state=initial_state)\n","        \n","        # 最後に隠れ層から出力層につながる重みとバイアスを処理する\n","        # 最終的に softmax 関数で処理し、確率として解釈される。\n","        # softmax 関数はこの関数の外で定義する。\n","        output = tf.matmul(outputs[-1], output_w) + output_b\n","\n","        return output\n","\n","    def loss(self, logits, labels):\n","        cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n","\n","        return cost\n","\n","    def training(self, cost):\n","        # 今回は最適化手法として Adam を選択する。\n","        # ここの AdamOptimizer の部分を変えることで、Adagrad, Adadelta などの他の最適化手法を選択することができる\n","        optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(cost)\n","\n","        return optimizer\n","\n","    def train(self):\n","        # 変数などの用意\n","        input_data = tf.placeholder(\"float\", [None, self.chunk_size, self.input_layer_size])\n","        actual_labels = tf.placeholder(\"float\", [None, self.output_layer_size])\n","        initial_state = tf.placeholder(\"float\", [None, self.hidden_layer_size])\n","\n","        prediction = self.inference(input_data, initial_state)\n","        cost = self.loss(prediction, actual_labels)\n","        optimizer = self.training(cost)\n","        correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(actual_labels, 1))\n","        accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n","\n","        # TensorBoard で可視化するため、クロスエントロピーをサマリーに追加\n","        tf.summary.scalar(\"Cross entropy: \", cost)\n","        summary = tf.summary.merge_all()\n","\n","        # 訓練・テストデータの用意\n","        # corpus = Corpus()\n","        trX, trY, teX, teY = self.corpus.prepare_data()\n","        training_num = trX.shape[0]\n","\n","        # ログを保存するためのディレクトリ\n","        timestamp = time.time()\n","        dirname = datetime.datetime.fromtimestamp(timestamp).strftime(\"%Y%m%d%H%M%S\")\n","\n","        # ここから実際に学習を走らせる\n","        with tf.Session() as sess:\n","            sess.run(tf.global_variables_initializer())\n","            summary_writer = tf.summary.FileWriter(\"./log/\" + dirname, sess.graph)\n","\n","            # エポックを回す\n","            for epoch in range(self.epochs):\n","                step = 0\n","                epoch_loss = 0\n","                epoch_acc = 0\n","\n","                # 訓練データをバッチサイズごとに分けて学習させる (= optimizer を走らせる)\n","                # エポックごとの損失関数の合計値や（訓練データに対する）精度も計算しておく\n","                while (step + 1) * self.batch_size < training_num:\n","                    start_idx = step * self.batch_size\n","                    end_idx = (step + 1) * self.batch_size\n","\n","                    batch_xs = trX[start_idx:end_idx, :, :]\n","                    batch_ys = trY[start_idx:end_idx, :]\n","\n","                    _, c, a = sess.run([optimizer, cost, accuracy],\n","                                       feed_dict={input_data: batch_xs,\n","                                                  actual_labels: batch_ys,\n","                                                  initial_state: np.zeros([self.batch_size, self.hidden_layer_size])\n","                                                  }\n","                                       )\n","                    epoch_loss += c\n","                    epoch_acc += a\n","                    step += 1\n","\n","                # コンソールに損失関数の値や精度を出力しておく\n","                print(\"Epoch\", epoch, \"completed ouf of\", self.epochs, \"-- loss:\", epoch_loss, \" -- accuracy:\",\n","                      epoch_acc / step)\n","\n","                # Epochが終わるごとにTensorBoard用に値を保存\n","                summary_str = sess.run(summary, feed_dict={input_data: trX,\n","                                                           actual_labels: trY,\n","                                                           initial_state: np.zeros(\n","                                                               [trX.shape[0],\n","                                                                self.hidden_layer_size]\n","                                                           )\n","                                                           }\n","                                       )\n","                summary_writer.add_summary(summary_str, epoch)\n","                summary_writer.flush()\n","\n","            # 学習したモデルも保存しておく\n","            saver = tf.train.Saver()\n","            saver.save(sess, self.model_filename)\n","\n","            # 最後にテストデータでの精度を計算して表示する\n","            a = sess.run(accuracy, feed_dict={input_data: teX, actual_labels: teY,\n","                                              initial_state: np.zeros([teX.shape[0], self.hidden_layer_size])})\n","            print(\"Accuracy on test:\", a)\n","\n","\n","    def predict(self, seq):\n","        \"\"\"\n","        文章を入力したときに次に来る単語を予測する\n","        :param seq: 予測したい単語の直前の文字列。chunk_size 以上の単語数が必要。\n","        :return:\n","        \"\"\"\n","\n","        # 最初に復元したい変数をすべて定義してしまいます\n","        tf.reset_default_graph()\n","        input_data = tf.placeholder(\"float\", [None, self.chunk_size, self.input_layer_size])\n","        initial_state = tf.placeholder(\"float\", [None, self.hidden_layer_size])\n","        prediction = tf.nn.softmax(self.inference(input_data, initial_state))\n","        predicted_labels = tf.argmax(prediction, 1)\n","\n","        # 入力データの作成\n","        # seq を one-hot 表現に変換する。\n","        words = [word for word in seq.split() if not word.startswith(\"-\")]\n","        x = np.zeros([1, self.chunk_size, self.input_layer_size])\n","        for i in range(self.chunk_size):\n","            word = seq[len(words) - self.chunk_size + i]\n","            index = self.dictionary.get(word, self.dictionary[self.unknown_word_symbol])\n","            x[0][i][index] = 1\n","        feed_dict = {\n","            input_data: x, # (1, chunk_size, vocabulary_size)\n","            initial_state: np.zeros([1, self.hidden_layer_size])\n","        }\n","\n","        # tf.Session()を用意\n","        with tf.Session() as sess:\n","            # 保存したモデルをロードする。ロード前にすべての変数を用意しておく必要がある。\n","            saver = tf.train.Saver()\n","            saver.restore(sess, self.model_filename)\n","\n","            # ロードしたモデルを使って予測結果を計算\n","            u, v = sess.run([prediction, predicted_labels], feed_dict=feed_dict)\n","\n","            keys = list(self.dictionary.keys())\n","\n","\n","            # コンソールに文字ごとの確率を表示\n","            for i in range(self.vocabulary_size):\n","                c = self.unknown_word_symbol if i == (self.vocabulary_size - 1) else keys[i]\n","                print(c, \":\", u[0][i])\n","\n","            print(\"Prediction:\", seq + \" \" + (\"<???>\" if v[0] == (self.vocabulary_size - 1) else keys[v[0]]))\n","\n","        return u[0]\n","\n","\n","def build_dict():\n","    cp = Corpus()\n","    cp.build_dict()\n","\n","if __name__ == \"__main__\":\n","    #build_dict()\n","\n","    ln = Language()\n","\n","    # 学習するときに呼び出す\n","    #ln.train()\n","\n","    # 保存したモデルを使って単語の予測をする\n","    ln.predict(\"some of them looks like\")"],"metadata":{"id":"wTJRVPN8V-rW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","from common import functions\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","\n","np.random.seed(0)\n","\n","# sin曲線\n","round_num = 10\n","div_num = 500\n","ts = np.linspace(0, round_num * np.pi, div_num)\n","f = np.sin(ts)\n","\n","def d_tanh(x):\n","    return 1/(np.cosh(x)**2 + 1e-4)\n","\n","# ひとつの時系列データの長さ\n","maxlen = 2\n","\n","# sin波予測の入力データ\n","test_head = [[f[k]] for k in range(0, maxlen)]\n","\n","data = []\n","target = []\n","\n","for i in range(div_num - maxlen):\n","    data.append(f[i: i + maxlen])\n","    target.append(f[i + maxlen])\n","    \n","X = np.array(data).reshape(len(data), maxlen, 1)\n","D = np.array(target).reshape(len(data), 1)\n","\n","# データ設定\n","N_train = int(len(data) * 0.8)\n","N_validation = len(data) - N_train\n","\n","x_train, x_test, d_train, d_test = train_test_split(X, D, test_size=N_validation)\n","\n","input_layer_size = 1\n","hidden_layer_size = 5\n","output_layer_size = 1\n","\n","weight_init_std = 0.01\n","learning_rate = 0.1\n","\n","iters_num = 500\n","\n","# ウェイト初期化 (バイアスは簡単のため省略)\n","W_in = weight_init_std * np.random.randn(input_layer_size, hidden_layer_size)\n","W_out = weight_init_std * np.random.randn(hidden_layer_size, output_layer_size)\n","W = weight_init_std * np.random.randn(hidden_layer_size, hidden_layer_size)\n","\n","# 勾配\n","W_in_grad = np.zeros_like(W_in)\n","W_out_grad = np.zeros_like(W_out)\n","W_grad = np.zeros_like(W)\n","\n","us = []\n","zs = []\n","\n","u = np.zeros(hidden_layer_size)\n","z = np.zeros(hidden_layer_size)\n","y = np.zeros(output_layer_size)\n","\n","delta_out = np.zeros(output_layer_size)\n","delta = np.zeros(hidden_layer_size)\n","\n","losses = []\n","\n","# トレーニング\n","for i in range(iters_num):\n","    for s in range(x_train.shape[0]):\n","        us.clear()\n","        zs.clear()\n","        z *= 0\n","        \n","        # sにおける正解データ\n","        d = d_train[s]\n","\n","        xs = x_train[s]        \n","        \n","        # 時系列ループ\n","        for t in range(maxlen):\n","            \n","            # 入力値\n","            x = xs[t]\n","            u = np.dot(x, W_in) + np.dot(z, W)\n","            us.append(u)\n","            z = np.tanh(u)\n","            zs.append(z)\n","\n","        y = np.dot(z, W_out)\n","        \n","        #誤差\n","        loss = functions.mean_squared_error(d, y)\n","        \n","        delta_out = functions.d_mean_squared_error(d, y)\n","        \n","        delta *= 0\n","        for t in range(maxlen)[::-1]:\n","            \n","            delta = (np.dot(delta, W.T) + np.dot(delta_out, W_out.T)) * d_tanh(us[t])\n","            \n","            # 勾配更新\n","            W_grad += np.dot(zs[t].reshape(-1,1), delta.reshape(1,-1))\n","            W_in_grad += np.dot(xs[t], delta.reshape(1,-1))\n","        W_out_grad = np.dot(z.reshape(-1,1), delta_out)\n","        \n","        # 勾配適用\n","        W -= learning_rate * W_grad\n","        W_in -= learning_rate * W_in_grad\n","        W_out -= learning_rate * W_out_grad.reshape(-1,1)\n","            \n","        W_in_grad *= 0\n","        W_out_grad *= 0\n","        W_grad *= 0\n","\n","# テスト        \n","for s in range(x_test.shape[0]):\n","    z *= 0\n","\n","    # sにおける正解データ\n","    d = d_test[s]\n","\n","    xs = x_test[s]\n","\n","    # 時系列ループ\n","    for t in range(maxlen):\n","\n","        # 入力値\n","        x = xs[t]\n","        u = np.dot(x, W_in) + np.dot(z, W)\n","        z = np.tanh(u)\n","\n","    y = np.dot(z, W_out)\n","\n","    #誤差\n","    loss = functions.mean_squared_error(d, y)\n","    print('loss:', loss, '   d:', d, '   y:', y)\n","        \n","        \n","        \n","original = np.full(maxlen, None)\n","pred_num = 200\n","\n","xs = test_head\n","\n","# sin波予測\n","for s in range(0, pred_num):\n","    z *= 0\n","    for t in range(maxlen):\n","        \n","        # 入力値\n","        x = xs[t]\n","        u = np.dot(x, W_in) + np.dot(z, W)\n","        z = np.tanh(u)\n","\n","    y = np.dot(z, W_out)\n","    original = np.append(original, y)\n","    xs = np.delete(xs, 0)\n","    xs = np.append(xs, y)\n","\n","plt.figure()\n","plt.ylim([-1.5, 1.5])\n","plt.plot(np.sin(np.linspace(0, round_num* pred_num / div_num * np.pi, pred_num)), linestyle='dotted', color='#aaaaaa')\n","plt.plot(original, linestyle='dashed', color='black')\n","plt.show()"],"metadata":{"id":"SlK4TKD8XeSI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 確認テスト"],"metadata":{"id":"va6EdomcGZIE"}},{"cell_type":"code","source":["サイズ5×5の入力画像を、サイズ3×3のフィルタで畳み込んだ時の出力画像のサイズを答えよ。なおストライドは2、パディングは1とする。\n","＝合格\n","＝（５＋２＊１−３）/2+1=3\n","\n","RNNのネットワークには大きくわけて3つの重みがある。1つは入力から現在の中間層を定義する際にかけられる重み、1つは中間層から出力を定義する際にかけられる重みである。残り1つの重みについて説明せよ。\n","=合格\n","＝前の中間層から次の中間層までの重み\n","\n","以下は再帰型ニューラルネットワークにおいて構文木を入力として再帰的に文全体の表現ベクトルを得るプログラムである。ただし、ニューラルネットワークの重みパラメータはグローバル変数として定義してあるものとし、_activation関数はなんらかの活性化関数であるとする。木構造は再帰的な辞書で定義してあり、rootが最も外側の辞書であると仮定する。\n","＝合格\n","＝正解２\n","\n","連鎖律の原理を使い、dz/dxを求めよ。\n","＝合格\n","\n","下図のy1をx・z0・z1・win・w・woutを用いて数式で表せ。※バイアスは任意の文字で定義せよ。※また中間層の出力にシグモイド関数g(x)を作用させよ。\n","＝合格\n","＝y1=g(z1*s1+c)=g(z1*(z0*w+x1*win+b)+c)\n","\n","左の図はBPTTを行うプログラムである。なお簡単化のため活性化関数は恒等関数であるとする。また、calculate_dout関数は損失関数を出力に関して偏微分した値を返す関数であるとする。（お）にあてはまるのはどれか。\n","=合格\n","＝正解は２。Uが隠れ層の重みであり、過去に遡るたびにそれをかけていかなければならない。\n","\n","シグモイド関数を微分した時、入力値が0の時に最大値をとる。その値として正しいものを選択肢から選べ。\n","＝合格\n","＝０．２５\n","\n","RNNや深いモデルでは勾配の消失または爆発が起こる傾向がある。勾配爆発を防ぐために勾配のクリッピングを行うという手法がある。具体的には勾配のノルムがしきい値を超えたら、勾配のノルムをしきい値に正規化するというものである。以下は勾配のクリッピングを行う関数である。（さ）にあてはまるのはどれか。（1）gradient * rate（2）gradient / norm（3）gradient / threshold（4）np.maximum(gradient, threshold)\n","＝合格\n","＝クリッピングは勾配の上限値を定義する\n","\n","以下の文章をLSTMに入力し空欄に当てはまる単語を予測したいとする。文中の「とても」という言葉は空欄の予測においてなくなっても影響を及ぼさないと考えられる。このような場合、どのゲートが作用すると考えられるか。「映画おもしろかったね。ところで、とてもお腹が空いたから何か____。」（3分）確認テスト\n","＝合格\n","＝忘却ゲート\n","\n","以下のプログラムはLSTMの順伝播を行うプログラムである。ただし_sigmoid関数は要素ごとにシグモイド関数を作用させる関数である。（け）にあてはまるのはどれか。\n","＝合格\n","\n","LSTMとCECが抱える課題について、それぞれ簡潔に述べよ。\n","＝合格\n","＝CECは学習機能がなく、LSTMはそれを補うために３つのゲート機能をもつが、その分計算負荷が大きくなる。\n","\n","GRU(Gated Recurrent Unit)もLSTMと同様にRNNの一種であり、単純なRNNにおいて問題となる勾配消失問題を解決し、長期的な依存関係を学習することができる。LSTMに比べ変数の数やゲートの数が少なく、より単純なモデルであるが、タスクによってはLSTMより良い性能を発揮する。以下のプログラムはGRUの順伝播を行うプログラムである。ただし_sigmoid関数は要素ごとにシグモイド関数を作用させる関数である。（こ）にあてはまるのはどれか。\n","＝合格\n","＝新しい中間状態は、1ステップ前の中間表現と計算された中間表現の線形和で宣言される。\n","\n","LSTMとGRUの違いを簡潔に述べよ。\n","＝合格\n","＝LSTMよりゲート機能を少なくし計算負荷を低めたのがGRUである。\n","\n","以下は双方向RNNの順伝播を行うプログラムである。順方向については、入力から中間層への重みW_f, 一ステップ前の中間層出力から中間層への重みをU_f、逆方向に関しては同様にパラメータW_b, U_bを持ち、両者の中間層表現を合わせた特徴から出力層への重みはVである。_rnn関数はRNNの順伝播を表し中間層の系列を返す関数であるとする。（か）にあてはまるのはどれか（1）h_f + h_b[::-1]（2）h_f * h_b[::-1]（3）np.concatenate([h_f, h_b[::-1]], axis=0)（4）np.concatenate([h_f, h_b[::-1]], axis=1)\n","＝合格\n","＝numpyのaxisは０が縦方向で、１が横方向である。\n","\n","下記の選択肢から、seq2seqについて説明しているものを選べ。（1）時刻に関して順方向と逆方向のRNNを構成し、それら2つの中間層表現を特徴量として利用するものである。（2）RNNを用いたEncoder-Decoderモデルの一種であり、機械翻訳などのモデルに使われる。（3）構文木などの木構造に対して、隣接単語から表現ベクトル（フレーズ）を作るという演算を再帰的に行い（重みは共通）、文全体の表現ベクトルを得るニューラルネットワークである。（4）RNNの一種であり、単純なRNNにおいて問題となる勾配消失問題をCECとゲートの概念を導入することで解決したものである。（3分）確認テスト\n","＝合格\n","＝１、双方向RNN：３、RNN：４，LSTM\n","\n","機械翻訳タスクにおいて、入力は複数の単語から成る文（文章）であり、それぞれの単語はone-hotベクトルで表現されている。Encoderにおいて、それらの単語は単語埋め込みにより特徴量に変換され、そこからRNNによって（一般にはLSTMを使うことが多い）時系列の情報をもつ特徴へとエンコードされる。以下は、入力である文（文章）を時系列の情報をもつ特徴量へとエンコードする関数である。ただし_activation関数はなんらかの活性化関数を表すとする。（き）にあてはまるのはどれか。\n","＝合格\n","\n","seq2seqとHRED、HREDとVHREDの違いを簡潔に述べよ。\n","＝seq2seqは入力されたテキストデータに対して出力するため、会話等の文脈を考慮した学習はできない。HREDはseq2seqにcontextRNNをとりいれ向上したものの、情報量にとぼしい応答を学習する傾向になった。\n","＝VHREDはVAEの潜在変数の概念を取り入れた。\n","\n","VAEに関する下記の説明文中の空欄に当てはまる言葉を答えよ。自己符号化器の潜在変数に____を導入したもの。\n","＝合格\n","＝標準正規分布\n","\n","RNNとword2vec、seq2seqとAttentionの違いを簡潔に述べよ。\n","＝合格\n","＝RNNは時系列に対応した学習手法、word2vecは単語の分散表現を得る手法。\n","＝seq2seqは系列データから系列データに変換する、Attentionは入力と出力の関連について重みをつける。\n"],"metadata":{"id":"u-MYu0r3Gidd"},"execution_count":null,"outputs":[]}]}