線形回帰モデル
１，要点
線形回帰とは、ある入力から主力を予測する回帰問題を線形モデルをつかってとく機械学習の一つである。
線形回帰のパラメータは最小二乗法で決定していく。また、誤差を正規分布に従う確率変数を仮定し尤度関数の最大化を利用した推定も可能である。
２，実装演習結果

非線形回帰モデル
１，要点
複雑な非線形構造を内在する現象に対して、非線形回帰モデリングを実施する。
基底展開法がよく使われ、基底関数として多項式関数、ガウス関数、スプライン関数などがよく使われる。
過学習対策として、L1ノルム（ラッソ）L2ノルム（リッジ）を使用した正則化法を使う場合もある。
２，実装演習結果

ロジスティックモデル
１，要点
分類問題を解くための教師あり機械学習モデルである。
ロジスティック回帰モデルではベルヌーイ分布（コイン投げ）を利用する。
尤度関数が最大となるパラメータを探すことになり、対数尤度関数を使用する。
recallは実際にtrueであるものうち正であると予測されたもの、precisionは正と予測されたものうち実際に正であるもの、F値はrecallとprecisionの調和平均
２，実装演習結果

主成分分析
１，要点
主成分分析は多変量データの持つ構造をより少数個の指標に圧縮する。
制約つき最適化問題を解くことになるが、その解は元のデータの分散共分散行列の固有値と固有ベクトルになる。
主成分分析の解釈としては、寄与率が大事でありデータの何割を説明することができているかを表している。
2,実装演習結果


アルゴリズム（knn,kmeans）
１，要点
ｋ近傍法とは、教師あり学習の一つであり、最近傍のデータを個取ってきて、それらがもっとも多く所属するクラスに識別する。
ハイパーパラメータであるkを大きくすると決定境界は滑らかになる。
ｋmeansは、教師なし学習手法であり、クラスタリングを行う。与えられたデータをk個のクラスタに分類する。
初期値が離れていないとうまくいかないことがある。
2,実装演習結果


サポートベクターマシン
１，要点
サポートベクターマシーンは教師あり学習である。
分類境界を挟んで各クラスがどのくらい離れているかをマージンとよび、このマージン最大化を行う。
分類境界に最も近いデータをサポートベクトルという。
ハードマージンでは訓練データを完璧に分類できるという仮定がある。
一方、ソフトマージンではマージン内に入るデータや誤分類されたデータに対する誤差を表す変数であるスラック変数を用いて誤分類を許容している。


２，実装演習結果
