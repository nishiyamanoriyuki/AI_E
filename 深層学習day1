Section1) 入力層〜中間層
要点まとめ
入力層から中間層への流れは以下の通りである。入力された値と重みの積をとり、バイアスを足して活性化関数に通す。この値を次の層への入力として処理する。
機械学習ではこの重みおよびバイアスを適切化していくことが目的となる。
pythonでの実装はnumpyを使用してu1 = np.dot(x, W1) + b1と表す。

Section2) 活性化関数
要点まとめ
活性化関数とは次の層への出力の大きさを決める非線形の関数である。入力値によって、次の層への信号のON/OFFや強弱を定める働きをもつ。
中間層への活性化関数としては、relu,シグモイド、ステップ関数などが使われ、出力層への活性化関数としてはシグモイド、ソフトマックス、恒等関数などが使われる。
特にreluが使われることが多い。


Section3) 出力層
要点まとめ
出力層では正解データとの誤差を算出する。出力層の表現はone-hotベクトルを使うことが多い。
出力層での活性化関数としてはソフトマックス関数、恒等写像、シグモイド関数（ロジスティック関数）を使用する。
分類問題では誤差関数は交差エントロピーを使用することが多い。

Section4) 勾配降下法
要点まとめ
誤差を最小化する重みを見つけるため、勾配降下法を使用する。
学習率がハイパーパラメータとして存在し、収束性向上アルゴリズムとしてMomentum、AdaGrad、Adadelta、Adamが採用されている。
SGDやミニバッチ勾配降下法を用いて、全データを一度に使用しない学習方法もある。

Section5) 誤差逆伝播法
要点まとめ


