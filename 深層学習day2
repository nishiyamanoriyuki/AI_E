section1:勾配消失問題
要点まとめ
勾配消失問題とは、下位層がパラメータはほとんど変わらず、最適値収束しなくなることである。
これは誤差逆伝播法が下位層に進んでいくに連れて、勾配がどんどん緩やかになっていくためである。
対策としては、活性化関数の選択、重みの初期値設定、バッチ正規化がある。
具体的にはrelu関数を使用する、xavierやhe(重みの要素を、前の層のノード数の平方根で除算等)の初期化法、バッチ正規化を行う。


section２:学習率最適化手法
要点まとめ
学習率最適化手法としてモメンタム•AdaGrad•RMSProp•Adamがある。
モーメンタムは局所的最適解にはならず、大域的最適解となる。・谷間についてから最も低い位置(最適値)にいくまでの時間が早い。
AdaGradは誤差を学習率が徐々に小さくなるので、鞍点問題を引き起こす。
RMSPropはハイパーパラメータの調整が必要な場合が少ない。
AdamはモメンタムおよびRMSPropのメリットをもっている。

section３:過学習
要点まとめ
過学習は重みが大きい値をとることで、過学習が発生することがある。
過学習抑制手法として、L1正則化、L2正則化、ドロップアウトがある。
L1正則化（ラッソ）は、ベクトル成分の絶対値の和(マンハッタン距離と呼ばれる)を使用し、L2正則化（リッジ）はユーグリッド距離を使用する。どちらも誤差関数にノルムを加える。
ドロップアウトはノードの数が多いことで起こる過学習を抑制する。



section４:畳み込みニューラルネットワークの概念
要点まとめ
CNNは畳み込み層とプーリング層によって構成される。


section５:最新のCNN
要点まとめ

