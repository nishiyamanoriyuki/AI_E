section1:勾配消失問題
要点まとめ
勾配消失問題とは、下位層がパラメータはほとんど変わらず、最適値収束しなくなることである。
これは誤差逆伝播法が下位層に進んでいくに連れて、勾配がどんどん緩やかになっていくためである。
対策としては、活性化関数の選択、重みの初期値設定、バッチ正規化がある。
具体的にはrelu関数を使用する、xavierやhe(重みの要素を、前の層のノード数の平方根で除算等)の初期化法、バッチ正規化を行う。


section２:学習率最適化手法
要点まとめ
学習率最適化手法としてモメンタム•AdaGrad•RMSProp•Adamがある。
モーメンタムは局所的最適解にはならず、大域的最適解となる。・谷間についてから最も低い位置(最適値)にいくまでの時間が早い。
AdaGradは誤差を学習率が徐々に小さくなるので、鞍点問題を引き起こす。
RMSPropはハイパーパラメータの調整が必要な場合が少ない。
AdamはモメンタムおよびRMSPropのメリットをもっている。

section３:過学習
要点まとめ


section４:畳み込みニューラルネットワークの概念
要点まとめ


section５:最新のCNN
要点まとめ

