section1:再帰型ニューラルネットワークの概念
RNNとは時系列データに対応可能な、ニューラルネットワークである。
初期の状態と過去の時間t-1の状態を保持し、そこから次の時間でのtを再帰的に求める再帰構造をもつ。
BPTTとはRNNにおける誤差逆伝播法の一種である。BPTTでは計算結果（=誤差）から微分を逆算することで、不要な再帰的計算を避けて微分を算出できる。

section２:LSTM
RNNは時間を遡るほど勾配が消失していくのでLSTMはそれを解決する。
LSTMの特徴として、CEC、入力ゲートと出力ゲート、忘却ゲート、覗き穴結合がある。
CECは勾配消失または爆発問題に対して勾配を１にすることで解決した。
覗き穴結合とはCEC自身の値に、重み行列を介して伝播可能にした構造である。

section３:GRU
LSTMは、パラメータが多数存在していたため、計算負荷が大きい。
GRUは、パラメータを大幅に削減し、精度は同等またはそれ以上が望める様になった構造。
GRUは「リセットゲート」と入力ゲートと出力ゲートをあわせた「更新ゲート」を持つ。

section４:双方向RNN
双方向RNNとは過去の情報だけでなく、未来の情報を加味することで、精度を向上させるためのモデルである。
双方向RNNの注意点は、未来の情報が分かっていなければ使えない。
文章の推敲や、機械翻訳、フレーム間の補完などのタスクに使用する。

section５:seq2seq
Seq2seqhはEncoder-Decoderモデルの一種で機械翻訳や機械対話に用いられる。

Encoder
Takingは文章を単語等のトークン毎に分割し、トークンごとのIDに分割する。
EmbeddingはIDから、そのトークンを表す分散表現ベクトルに変換する。
Encoder RNNではベクトルを順番にRNNに入力していく。

Decoder
Decoder RNNでは Encoder RNN のfinal stateからEmbedding を入力
生成確率にもとづいてtoken をランダムにSamplingする
token をEmbedding してDecoder RNN への次の入力にする
上記を繰り返してtoken を文字列になおす（Detokenize）

HRED
Seq2Seq+ Context RNNdで過去の発話の履歴を加味した返答をできる。
Context RNNはEncoder のまとめた各文章の系列をまとめて、これまでの会話コンテキスト全体を表すベクトルに変換する。

VHRED
VHREDはHREDに、VAEの潜在変数の概念を追加した。
VAEは潜在変数zに確率分布z∼N(0,1)を仮定したものである。

section6:word2vec
word2vecはone_hotベクトル表現からembedding表現を得るためのもの。


section7:attention mechanism
