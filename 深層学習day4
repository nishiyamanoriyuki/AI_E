section1:強化学習
強化学習は行動の結果として与えられる利益(報酬)をもとに、行動を決定する原理を改善していく。
Q学習と関数近似法を組み合わせる手法を使用する。Q学習は行動価値関数を行動する毎に更新することにより学習を進める方法。
関数近似法は価値関数や方策関数を関数近似する手法。

section２:alpha go
alpha goには、alphago(lee)とalphago(zero)がある。

alphago(lee)は、RollOutPolicy（＝出力はそのマスの着手予想確率）、PolicyNet（＝出力は19×19マスの着手予想確率）,ValueNet（＝出力は現局面の勝率）の学習をする。
教師あり学習によるRollOutPolicyとPolicyNetの学習、強化学習によるPolicyNetの学習、強化学習によるValueNetの学習を行う。
強化学習の手法としてモンテカルロ木探索を使用する。
AlphaGoZeroの特徴は、強化学習のみで作成、特徴入力からヒューリスティックな要素を排除、PolicyNetとValueNetを１つのネットワークに統合、Residual Netの導入、RollOut方策を使用しない。

section3:軽量化高速化技術
軽量化高速化技術として、データ並列化、モデル並列化、GPUの使用、モデルの軽量化（量子化・蒸留・プルーニング）がある。
データ並列化は同期型と非同期型があり、同期型が現在の主流となる。
GPGPUの開発環境としてCUDAやopenclがある。
量子化は通常のパラメータの64 bit 浮動小数点を32 bit など下位の精度に落とす。
蒸留は、学習済みの精度の高いモデルの知識を軽量なモデルへ継承させる。教師モデルの重みを固定し生徒モデルの重みを更新していく。
プルーニングは重みが閾値以下のニューロンを削減する。

section４:応用モデル
MobileNetsはDepthwiseConvolutionとPointwise Convolutionの組み合わせで軽量化を実現した。
Depthwise Convolutionはチャンネル数とフィルタ数が１であり、Pointwise Convolutionはフィルタサイズが１＊１である。
DenseNetは前方の各層からの出力全てが後方の層への入力として用いられる。
成⻑率(Growth Rate)と呼ばれるハイパーパラメータが存在し、各ブロック毎にk個ずつ特徴マップのチャネル数が増加していく。
Transition Layerと呼ばれる層でダウンサンプリングをしてDence block同士をつなぐ。
Batch Norm Layerはレイヤー間を流れるデータの分布を、ミニバッチ単位で平均が0・分散が1になるように正規化する。
Layer NormはH x W x Cの全てのpixelを正規化する。
Instance Normは各sampleの各チャンネルごとに正規化する。
WaveNetは時系列データに対して畳み込み（Dilated convolution）を使う。（層が深くなるにつれて畳み込みリンクを離し、パラメータ数に対する受容野が広い。）

section５:Transformer



section6:物体検知・セグメンテーション




